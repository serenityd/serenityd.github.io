<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python学习笔记（十）类与对象]]></title>
    <url>%2F2018-3-25-one%2F</url>
    <content type="text"><![CDATA[1.1定义类定义一个类，格式如下： class 类名: 方法列表demo：定义一个Car类12345678# 定义类class Car: # 方法 def getCarInfo(self): print('车轮子个数:%d, 颜色%s'%(self.wheelNum, self.color)) def move(self): print("车正在移动...") 说明： 定义类时有2种：新式类和经典类，上面的Car为经典类，如果是Car(object)则为新式类类名 的命名规则按照”大驼峰” 1.2创建一个对象通过上一节课程，定义了一个Car类；就好比有车一个张图纸，那么接下来就应该把图纸交给生成工人们去生成了 python中，可以根据已经定义的类去创建出一个个对象 创建对象的格式为: 对象名 = 类名()创建对象demo:1234567891011121314151617181920# 定义类class Car: # 移动 def move(self): print('车在奔跑...') # 鸣笛 def toot(self): print("车在鸣笛...嘟嘟..")# 创建一个对象，并用变量BMW来保存它的引用BMW = Car()BMW.color = '黑色'BMW.wheelNum = 4 #轮子数量BMW.move()BMW.toot()print(BMW.color)print(BMW.wheelNum) 总结： BMW = Car()，这样就产生了一个Car的实例对象，此时也可以通过实例对象BMW来访问属性或者方法第一次使用BMW.color = ‘黑色’表示给BMW这个对象添加属性，如果后面再次出现BMW.color = xxx表示对属性进行修改BMW是一个对象，它拥有属性（数据）和方法（函数）当创建一个对象时，就是用一个模子，来制造一个实物 想一想: 在上一小节的demo中，我们已经给BMW这个对象添加了2个属性，wheelNum（车的轮胎数量）以及color（车的颜色），试想如果再次创建一个对象的话，肯定也需要进行添加属性，显然这样做很费事，那么有没有办法能够在创建对象的时候，就顺便把车这个对象的属性给设置呢？答: 2.1 init()方法init()方法 使用方式 def 类名: #初始化函数，用来完成一些默认的设定 def __init__(): pass init()方法的调用123456789101112131415# 定义汽车类class Car: def __init__(self): self.wheelNum = 4 self.color = '蓝色' def move(self): print('车在跑，目标:夏威夷')# 创建对象BMW = Car()print('车的颜色为:%s'%BMW.color)print('车轮胎数量为:%d'%BMW.wheelNum) 总结1 当创建Car对象后，在没有调用init()方法的前提下，BMW就默认拥有了2个属性wheelNum和color，原因是init()方法是在创建对象后，就立刻被默认调用了想一想 既然在创建完对象后init()方法已经被默认的执行了，那么能否让对象在调用init()方法的时候传递一些参数呢？如果可以，那怎样传递呢？1234567891011121314151617181920# 定义汽车类class Car: def __init__(self, newWheelNum, newColor): self.wheelNum = newWheelNum self.color = newColor def move(self): print('车在跑，目标:夏威夷')# 创建对象BMW = Car(4, 'green')print('车的颜色为:%s'%BMW.color)print('车轮子数量为:%d'%BMW.wheelNum)总结2__init__()方法，在创建一个对象时默认被调用，不需要手动调用__init__(self)中，默认有1个参数名字为self，如果在创建对象时传递了2个实参，那么__init__(self)中出了self作为第一个形参外还需要2个形参，例如__init__(self,x,y)__init__(self)中的self参数，不需要开发者传递，python解释器会自动把当前的对象引用传递进去 2.2str()方法 打印id() 如果把BMW使用print进行输出的话，会看到如下的信息 即看到的是创建出来的BMW对象在内存中的地址 定义str()方法1234567891011121314151617class Car: def __init__(self, newWheelNum, newColor): self.wheelNum = newWheelNum self.color = newColor def __str__(self): msg = "嘿。。。我的颜色是" + self.color + "我有" + int(self.wheelNum) + "个轮胎..." return msg def move(self): print('车在跑，目标:夏威夷')BMW = Car(4, "白色")print(BMW)总结 在python中方法名如果是xxxx()的，那么就有特殊的功能，因此叫做“魔法”方法当使用print输出对象的时候，只要自己定义了str(self)方法，那么就会打印从在这个方法中return的数据创建对象后，python解释器默认调用init()方法； 2.3str()方法当删除一个对象时，python解释器也会默认调用一个方法，这个方法为del()方法123456789101112131415161718192021222324252627282930313233343536import timeclass Animal(object): # 初始化方法 # 创建完对象后会自动被调用 def __init__(self, name): print('__init__方法被调用') self.__name = name # 析构方法 # 当对象被删除时，会自动被调用 def __del__(self): print("__del__方法被调用") print("%s对象马上被干掉了..."%self.__name)# 创建对象dog = Animal("哈皮狗")# 删除对象del dogcat = Animal("波斯猫")cat2 = catcat3 = catprint("---马上 删除cat对象")del catprint("---马上 删除cat2对象")del cat2print("---马上 删除cat3对象")del cat3print("程序2秒钟后结束")time.sleep(2) 结果:总结 当有1个变量保存了对象的引用时，此对象的引用计数就会加1 当使用del删除变量指向的对象时，如果对象的引用计数不会1，比如3，那么此时只会让这个引用计数减1，即变为2，当再次调用del时，变为1，如果再调用1次del，此时会真的把对象进行删除 3.1 self的用处 理解self 看如下示例:1234567891011121314151617181920# 定义一个类class Animal: # 方法 def __init__(self, name): self.name = name def printName(self): print('名字为:%s'%self.name)# 定义一个函数def myPrint(animal): animal.printName()dog1 = Animal('西西')myPrint(dog1)dog2 = Animal('北北')myPrint(dog2) 总结 所谓的self，可以理解为自己可以把self当做C++中类里面的this指针一样理解，就是对象自身的意思某个对象调用其方法时，python解释器会把这个对象作为第一个参数传递给self，所以开发者只需要传递后面的参数即可 3.3私有属性如果有一个对象，当需要对其进行修改属性时，有2种方法 对象名.属性名 = 数据 —-&gt;直接修改 对象名.方法名() —-&gt;间接修改为了更好的保存属性安全，即不能随意修改，一般的处理方式为 将属性定义为私有属性 添加一个可以调用的方法，供调用12345678910111213141516171819202122232425262728293031323334353637class People(object): def __init__(self, name): self.__name = name def getName(self): return self.__name def setName(self, newName): if len(newName) &gt;= 5: self.__name = newName else: print("error:名字长度需要大于或者等于5")xiaoming = People("dongGe")print(xiaoming.__name)class People(object): def __init__(self, name): self.__name = name def getName(self): return self.__name def setName(self, newName): if len(newName) &gt;= 5: self.__name = newName else: print("error:名字长度需要大于或者等于5")xiaoming = People("dongGe")xiaoming.setName("wanger")print(xiaoming.getName())xiaoming.setName("lisi")print(xiaoming.getName()) 总结 Python中没有像C++中public和private这些关键字来区别公有属性和私有属性 它是以属性命名方式来区分，如果在属性名前面加了2个下划线’__’，则表明该属性是私有属性，否则为公有属性（方法也是一样，方法名前面加了2个下划线的话表示该方法是私有的，否则为公有的）。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（九）文件的打开与关闭]]></title>
    <url>%2F2018-3-17-two%2F</url>
    <content type="text"><![CDATA[想一想：如果想用word编写一份简历，应该有哪些流程呢？ 打开word软件，新建一个word文件 写入个人简历信息 保存文件 关闭word软件同样，在操作文件的整体过程与使用word编写一份简历的过程是很相似的 （io操作） 打开文件，或者新建立一个文件 读/写数据 关闭文件 打开文件在python，使用open函数，可以打开一个已经存在的文件，或者创建一个新文件open(文件名，访问模式)示例如下：f = open(‘test.txt’,’w’)说明: ‘r’ open for reading (default) ‘w’ open for writing, truncating the file first ‘x’ open for exclusive creation, failing if the file already exists ‘a’ open for writing, appending to the end of the file if it exists ‘b’ binary mode ‘t’ text mode (default) ‘+’ open a disk file for updating (reading and writing) ‘U’ universal newlines mode (deprecated) 关闭文件close( )示例如下：12345#新建一个文件，文件名为:test.txt f = open('test.txt','w')#关闭这个文件 f.close() 写数据(write)使用write()可以完成向文件写入数据demo:123f = open('test.txt','w')f.write('helloworld, i am here!')f.close() 注意：如果文件不存在那么创建，如果存在那么就先清空，然后写入数据 读数据(read)使用read(num)可以从文件中读取数据，num表示要从文件中读取的数据的长度（单位是字节），如果没有传入num，那么就表示读取文件中所有的数据demo:1234567f = open('test.txt','r')content = f.read(1024)print(content)print("-"*30)content= f.read()print(content)f.close() 注意： 如果open是打开一个文件，那么可以不用写打开的模式，即只写 open(‘test.txt’) 如果使用读了多次，那么后面读取的数据是从上次读完后的位置开始的 应用1:文件的copy 读数据（readlines）就像read没有参数时一样，readlines可以按照行的方式把整个文件中的内容进行一次性读取，并且返回的是一个列表，其中每一行的数据为一个元素12345678f = open('test.txt','r')content = f.readlines()print(type(content))i=1for temp in content: print("%d:%s"%(i,temp)) i+=1f.close() 读数据（readline）123456f = open('test.txt','r')content = f.readline()print("1:%s"%content)content= f.readline()print("2:%s"%content)f.close() 想一想：如果一个文件很大，比如5G，试想应该怎样把文件的数据读取到内存然后进行处理呢？ 文件的定位读写 获取当前读写的位置tell在读写文件的过程中，如果想知道当前的位置，可以使用tell()来获取 定位到某个位置如果在读写文件的过程中，需要从另外一个位置进行操作的话，可以使用seek()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（八）匿名函数]]></title>
    <url>%2F2018-3-17-one%2F</url>
    <content type="text"><![CDATA[用lambda关键词能创建小型匿名函数。这种函数得名于省略了用def声明函数的标准步骤。 lambda函数的语法只包含一个语句，如下： lambda [arg1 [,arg2,.....argn]]:expression 如下实例：1234sum = lambda arg1, arg2: arg1 + arg2#调用sum函数print "Value of total : ", sum( 10, 20 )print "Value of total : ", sum( 20, 20 ) 以上实例输出结果： Value of total : 30 Value of total : 40 Lambda函数能接收任何数量的参数但只能返回一个表达式的值 匿名函数不能直接调用print，因为lambda需要一个表达式 应用场合函数作为参数传递自己定义函数123456789&gt;&gt;&gt; def fun(a, b, opt):... print "a =", a... print "b =", b... print "result =", opt(a, b)...&gt;&gt;&gt; fun(1, 2, lambda x,y:x+y)a = 1b = 2result = 3 作为内置函数的参数想一想，下面的数据如何指定按age或name排序？12345stus = [ &#123;"name":"zhangsan", "age":18&#125;, &#123;"name":"lisi", "age":19&#125;, &#123;"name":"wangwu", "age":17&#125;] 按name排序：123&gt;&gt;&gt; stus.sort(key = lambda x:x['name'])&gt;&gt;&gt; stus[&#123;'age': 19, 'name': 'lisi'&#125;, &#123;'age': 17, 'name': 'wangwu'&#125;, &#123;'age': 18, 'name': 'zhangsan'&#125;] 按age排序：123&gt;&gt;&gt; stus.sort(key = lambda x:x['age'])&gt;&gt;&gt; stus[&#123;'age': 17, 'name': 'wangwu'&#125;, &#123;'age': 18, 'name': 'zhangsan'&#125;, &#123;'age': 19, 'name': 'lisi'&#125;]]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2018-2-4-two%2F</url>
    <content type="text"><![CDATA[备用快速传送门]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令图示]]></title>
    <url>%2F2018-2-4-one%2F</url>
    <content type="text"><![CDATA[vi命令图示 备用]]></content>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下搭建python3环境]]></title>
    <url>%2F2018-2-3-two%2F</url>
    <content type="text"><![CDATA[python3环境设置printenv发现环境PATH中并没有python修改个人环境变量设置vim ~/.bashrc添加python bin目录到export PATH=/usr/python3.6.4/bin:$PATH生效方法:输入source ~/.bashrc命令，立即生效linux使用CentOS6.5以后用到会更新一些linxu命令备用python环境整个采用Xshell+VMwork+CentOs环境 配置不难]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客在VSCode下编写及git配置问题]]></title>
    <url>%2F2018-2-3-one%2F</url>
    <content type="text"><![CDATA[设置VSCode下git环境文件—首选项-设置用户设置下 添加git路径以及git-bash终端1234&#123; "git.path": "D:\\Program Files\\Git\\cmd", "terminal.integrated.shell.windows":"D:\\ProgramFiles\\Git\\bin\\bash.exe",&#125; PS:记录两天搭建环境过程中遇到的各种麻烦 好久不弄又出现一些新问题一、图片加载问题下面给出hexo图片公式加载环境设置经过一番折腾发现加载图片必须满足一下条件1、_config.yml中开启文件夹声明post_asset_folder: true2、npm安装图片插件npm install hexo-asset-image --save图片测试二、公式加载问题此处分为两个小问题一是公式插件加载问题二是markdown斜线注释问题1、公式插件按照网上教程加载hexo-math不成功，所以另辟蹊径采用mathjax.ejs文件建立公式环境到主题目录下mathjax.ejs:12345678910111213141516171819&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config(&#123; tex2jax: &#123; inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;&#125;);MathJax.Hub.Queue(function() &#123; var all = MathJax.Hub.getAllJax(), i; for(i=0; i &lt; all.length; i += 1) &#123; all[i].SourceElement().parentNode.className += ' has-jax'; &#125; &#125;);&lt;/script&gt;&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; src地址需要更新到mathjax接口最新地址然后加载启用判断在after_footer.ejs中123&lt;% if (page.mathjax)&#123; %&gt;&lt;%- partial('mathjax') %&gt;&lt;% &#125; %&gt; 2、斜线注释问题在node_modules\marked\lib\marked.js中修改两处代码这样无需替换markdown环境将451行和459行的12escape: /^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/ em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 替换为12escape: /^\\([`*\[\]()# +\-.!_&gt;])/em:/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 这样文章只要开启公式就可以加载公式公式 test$a^2=b^2+c^2$]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git学习笔记]]></title>
    <url>%2F2017-11-10-one%2F</url>
    <content type="text"><![CDATA[ubuntu下用hexo写博客,借机学习了一下git,感觉还是很强大的管理系统,这里记录一下git常用的基础命令,随时添加.工作区 暂存区 仓库关系 //初始化git git init //添加文件到暂存区 git add. //提交文件到仓库 git commit -m "x" //查看工作状态 git status //比较不同 git diff //查看日志 git log git log --pretty=oneline //回退一个版本 git reset --hard HEAD^ //回退版本 编号 3628164 git reset --hard 3628164 //命令记录 git reflog //创建SSH Key id_rsa.pub公钥 ssh-keygen -t rsa -C "youremail@example.com" //关联远程仓库 git remote add origin git@github.com:michaelliao/learngit.git //推送master到origin git push origin master //同步远程到master git pull origin master //同步到本地 git clone git@github.com:michaelliao/gitskills.git //创建+切换分支 git checkout -b &lt;name&gt; //查看分支 git branch //切换分支 git checkout &lt;name&gt;]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二十三） hierarchical clustering 层次聚类应用（完）]]></title>
    <url>%2F2017-9-21-two%2F</url>
    <content type="text"><![CDATA[hierarchical clustering聚类算法python代码：HierarchicalClustering.py from numpy import * """ Code for hierarchical clustering, modified from Programming Collective Intelligence by Toby Segaran (O'Reilly Media 2007, page 33). """ class cluster_node: def __init__(self,vec,left=None,right=None,distance=0.0,id=None,count=1): self.left=left self.right=right self.vec=vec self.id=id self.distance=distance self.count=count #only used for weighted average def L2dist(v1,v2): return sqrt(sum((v1-v2)**2)) def L1dist(v1,v2): return sum(abs(v1-v2)) # def Chi2dist(v1,v2): # return sqrt(sum((v1-v2)**2)) def hcluster(features,distance=L2dist): #cluster the rows of the "features" matrix distances={} currentclustid=-1 # clusters are initially just the individual rows clust=[cluster_node(array(features[i]),id=i) for i in range(len(features))] while len(clust)&gt;1: lowestpair=(0,1) closest=distance(clust[0].vec,clust[1].vec) # loop through every pair looking for the smallest distance for i in range(len(clust)): for j in range(i+1,len(clust)): # distances is the cache of distance calculations if (clust[i].id,clust[j].id) not in distances: distances[(clust[i].id,clust[j].id)]=distance(clust[i].vec,clust[j].vec) d=distances[(clust[i].id,clust[j].id)] if d&lt;closest: closest=d lowestpair=(i,j) # calculate the average of the two clusters mergevec=[(clust[lowestpair[0]].vec[i]+clust[lowestpair[1]].vec[i])/2.0 \ for i in range(len(clust[0].vec))] # create the new cluster newcluster=cluster_node(array(mergevec),left=clust[lowestpair[0]], right=clust[lowestpair[1]], distance=closest,id=currentclustid) # cluster ids that weren't in the original set are negative currentclustid-=1 del clust[lowestpair[1]] del clust[lowestpair[0]] clust.append(newcluster) return clust[0] def extract_clusters(clust,dist): # extract list of sub-tree clusters from hcluster tree with distance&lt;dist clusters = {} if clust.distance&lt;dist: # we have found a cluster subtree return [clust] else: # check the right and left branches cl = [] cr = [] if clust.left!=None: cl = extract_clusters(clust.left,dist=dist) if clust.right!=None: cr = extract_clusters(clust.right,dist=dist) return cl+cr def get_cluster_elements(clust): # return ids for elements in a cluster sub-tree if clust.id&gt;=0: # positive id means that this is a leaf return [clust.id] else: # check the right and left branches cl = [] cr = [] if clust.left!=None: cl = get_cluster_elements(clust.left) if clust.right!=None: cr = get_cluster_elements(clust.right) return cl+cr def printclust(clust,labels=None,n=0): # indent to make a hierarchy layout for i in range(n): print ' ', if clust.id&lt;0: # negative id means that this is branch print '-' else: # positive id means that this is an endpoint if labels==None: print clust.id else: print labels[clust.id] # now print the right and left branches if clust.left!=None: printclust(clust.left,labels=labels,n=n+1) if clust.right!=None: printclust(clust.right,labels=labels,n=n+1) def getheight(clust): # Is this an endpoint? Then the height is just 1 if clust.left==None and clust.right==None: return 1 # Otherwise the height is the same of the heights of # each branch return getheight(clust.left)+getheight(clust.right) def getdepth(clust): # The distance of an endpoint is 0.0 if clust.left==None and clust.right==None: return 0 # The distance of a branch is the greater of its two sides # plus its own distance return max(getdepth(clust.left),getdepth(clust.right))+clust.distance TestHClustering.py import os from PIL import Image , ImageDraw from HierarchicalClustering import hcluster from HierarchicalClustering import getheight from HierarchicalClustering import getdepth import numpy as np import os def drawdendrogram(clust,imlist, jpeg='clusters.jpg'): h=getheight(clust)*20 w=1200 depth=getdepth(clust) scaling=float(w-150)/depth img=Image.new('RGB',(w,h),(255, 255, 255)) draw=ImageDraw.Draw(img) draw.line((0,h/2,10,h/2),fill=(255,0,0)) drawnode(draw, clust, 10, int(h/2), scaling, imlist, img) img.save(jpeg) def drawnode(draw,clust, x, y, scaling,imlist,img): if clust.id&lt;0: h1=getheight(clust.left)*20 h2=getheight(clust.right)*20 top=y-(h1+h2)/2 bottom=y+(h1+h2)/2 ll=clust.distance*scaling draw.line((x,top+h1/2,x,bottom-h2/2),fill=(255,0,0)) draw.line((x,top+h1/2,x+ll,top+h1/2),fill=(255,0,0)) draw.line((x,bottom-h2/2,x+ll,bottom-h2/2),fill=(255,0,0)) drawnode(draw,clust.left,x+ll,top+h1/2,scaling,imlist,img) drawnode(draw,clust.right,x+ll,bottom-h2/2,scaling,imlist,img) else: nodeim=Image.open(imlist[clust.id]) nodeim.thumbnail((20,20)) ns=nodeim.size print x,y-ns[1]//2 print x+ns[0] print img.paste(nodeim,(int(x),int(y-ns[1]//2),int(x+ns[0]),int(y+ns[1]-ns[1]//2))) imlist=[] folderPath=r'C:\Users\Administrator\Desktop\picture' for filename in os.listdir(folderPath): if os.path.splitext(filename)[1]=='.jpg': imlist.append(os.path.join(folderPath,filename)) n=len(imlist) features = np.zeros((n,3)) for i in range(n): im=np.array(Image.open(imlist[i])) R=np.mean(im[:,:,0].flatten()) G=np.mean(im[:,:,1].flatten()) B=np.mean(im[:,:,2].flatten()) features[i]=np.array([R,G,B]) tree=hcluster(features) drawdendrogram(tree,imlist,jpeg='clusters.jpg') 归类结果：]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二十二）hierarchical clustering 层次聚类]]></title>
    <url>%2F2017-9-21-one%2F</url>
    <content type="text"><![CDATA[假设有N个待聚类的样本，对于层次聚类来说，步骤：1、（初始化）把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；2、寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；3、重新计算新生成的这个类与各个旧类之间的相似度；4、重复2和3直到所有样本点都归为一类，结束整个聚类过程其实是建立了一棵树，在建立的过程中，可以通过在第二步上设置一个阈值，当最近的两个类的距离大于这个阈值，则认为迭代可以终止。另外关键的一步就是第三步，如何判断两个类之间的相似度有不少种方法。这里介绍一下三种：SingleLinkage：又叫做 nearest-neighbor ，就是取两个类中距离最近的两个样本的距离作为这两个集合的距离，也就是说，最近两个样本之间的距离越小，这两个类之间的相似度就越大。容易造成一种叫做 Chaining 的效果，两个 cluster 明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster 。CompleteLinkage：这个则完全是 Single Linkage 的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个 cluster 即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点。Average-linkage：这种方法就是把两个集合中的点两两的距离全部放在一起求一个平均值，相对也能得到合适一点的结果。average-linkage的一个变种就是取两两距离的中值，与取均值相比更加能够解除个别偏离样本对结果的干扰。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二十一）聚类(Clustering) K-means算法应用]]></title>
    <url>%2F2017-9-18-one%2F</url>
    <content type="text"><![CDATA[KMeans聚类算法python代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as np# Function: K Means# -------------# K-Means is an algorithm that takes in a dataset and a constant# k and returns k centroids (which define clusters of data in the# dataset which are similar to one another).def kmeans(X, k, maxIt): numPoints, numDim = X.shape dataSet = np.zeros((numPoints, numDim + 1)) dataSet[:, :-1] = X # Initialize centroids randomly centroids = dataSet[np.random.randint(numPoints, size = k), :] centroids = dataSet[0:2, :] #Randomly assign labels to initial centorid centroids[:, -1] = range(1, k +1) # Initialize book keeping vars. iterations = 0 oldCentroids = None # Run the main k-means algorithm while not shouldStop(oldCentroids, centroids, iterations, maxIt): print "iteration: \n", iterations print "dataSet: \n", dataSet print "centroids: \n", centroids # Save old centroids for convergence test. Book keeping. oldCentroids = np.copy(centroids) iterations += 1 # Assign labels to each datapoint based on centroids updateLabels(dataSet, centroids) # Assign centroids based on datapoint labels centroids = getCentroids(dataSet, k) # We can get the labels too by calling getLabels(dataSet, centroids) return dataSet# Function: Should Stop# -------------# Returns True or False if k-means is done. K-means terminates either# because it has run a maximum number of iterations OR the centroids# stop changing.def shouldStop(oldCentroids, centroids, iterations, maxIt): if iterations &gt; maxIt: return True return np.array_equal(oldCentroids, centroids) # Function: Get Labels# -------------# Update a label for each piece of data in the dataset. def updateLabels(dataSet, centroids): # For each element in the dataset, chose the closest centroid. # Make that centroid the element's label. numPoints, numDim = dataSet.shape for i in range(0, numPoints): dataSet[i, -1] = getLabelFromClosestCentroid(dataSet[i, :-1], centroids) def getLabelFromClosestCentroid(dataSetRow, centroids): label = centroids[0, -1]; minDist = np.linalg.norm(dataSetRow - centroids[0, :-1]) for i in range(1 , centroids.shape[0]): dist = np.linalg.norm(dataSetRow - centroids[i, :-1]) if dist &lt; minDist: minDist = dist label = centroids[i, -1] print "minDist:", minDist return label # Function: Get Centroids# -------------# Returns k random centroids, each of dimension n.def getCentroids(dataSet, k): # Each centroid is the geometric mean of the points that # have that centroid's label. Important: If a centroid is empty (no points have # that centroid's label) you should randomly re-initialize it. result = np.zeros((k, dataSet.shape[1])) for i in range(1, k + 1): oneCluster = dataSet[dataSet[:, -1] == i, :-1] result[i - 1, :-1] = np.mean(oneCluster, axis = 0) result[i - 1, -1] = i return result x1 = np.array([1, 1])x2 = np.array([2, 1])x3 = np.array([4, 3])x4 = np.array([5, 4])testX = np.vstack((x1, x2, x3, x4))result = kmeans(testX, 2, 10)print "final result:"print result 打印结果均为[1,1],[2,1]为一组，[4，3][5,4]为一组，与手算结果一致]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二十）聚类(Clustering) K-means算法]]></title>
    <url>%2F2017-9-15-one%2F</url>
    <content type="text"><![CDATA[归类：聚类(clustering) 属于非监督学习 (unsupervised learning)无类别标记(class label) 举例： K-means 算法：3.1 Clustering 中的经典算法，数据挖掘十大经典算法之一3.2 算法接受参数 k ；然后将事先输入的n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。3.3 算法思想：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果3.4 算法描述：（1）适当选择c个类的初始中心；（2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的中心所在的类；（3）利用均值等方法更新该类的中心值；（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。3.5 算法流程：输入：k, data[n];（1） 选择k个初始中心点，例如c[0]=data[0],…c[k-1]=data[k-1];（2） 对于data[0]….data[n], 分别与c[0]…c[k-1]比较，假定与c[i]差值最少，就标记为i;（3） 对于所有标记为i点，重新计算c[i]={ 所有标记为i的data[j]之和}/标记为i的个数；（4） 重复(2)(3),直到所有c[i]值的变化小于给定阈值。 举例：停止优点：速度快，简单缺点：最终结果跟初始点选择相关，容易陷入局部最优，需直到k值Reference:http://croce.ggf.br/dados/K%20mean%20Clustering1.pdf]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十九）回归中的相关度和R平方值代码]]></title>
    <url>%2F2017-9-14-two%2F</url>
    <content type="text"><![CDATA[线性回归计算R平方值和相关度python代码：123456789101112131415161718192021222324252627282930313233343536import numpy as npfrom astropy.units import Ybarnimport mathdef computeCorrelation(X, Y): xBar = np.mean(X) yBar = np.mean(Y) SSR = 0 varX = 0 varY = 0 for i in range(0 , len(X)): diffXXBar = X[i] - xBar diffYYBar = Y[i] - yBar SSR += (diffXXBar * diffYYBar) varX += diffXXBar**2 varY += diffYYBar**2 SST = math.sqrt(varX * varY) return SSR / SSTdef polyfit(x,y,degree): results=&#123;&#125; coeffs=np.polyfit(x,y,degree) results['polinoial']=coeffs.tolist() p=np.poly1d(coeffs) yhat=p(x) ybar=np.mean(y) ssr=np.sum((yhat-ybar)**2) sst=np.sum((y-ybar)**2) results['determination']=ssr/sst; return results testX = [1, 3, 8, 7, 9]testY = [10, 12, 24, 21, 34]print computeCorrelation(testX, testY) **2print polyfit(testX, testY, 1) 打印结果均为0.884]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十八）回归中的相关度和R平方值]]></title>
    <url>%2F2017-9-14-one%2F</url>
    <content type="text"><![CDATA[皮尔逊相关系数 (Pearson Correlation Coefficient):1.1 衡量两个值线性相关强度的量1.2 取值范围 [-1, 1]: 正向相关: &gt;0, 负向相关：&lt;0, 无相关性：=01.3$\rho=Cor(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$$r_{xy}=\frac{\sum(x-\overline{x})(y-\overline{y})}{\sqrt{\sum(x-\overline{x})^2(y-\overline{y})^2}}$ 计算方法举例：X Y1 103 128 247 219 34 其他例子： R平方值:4.1 定义：决定系数，反应因变量的全部变异能通过回归关系被自变量解释的比例。4.2 描述：如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少80%4.3： 简单线性回归：R^2 = r * r 多元线性回归：$R^2=\frac{SSR}{SST}=\frac{\sum(\hat{y_i}-\overline{y})^2}{\sum(y_i-\overline{y})^2}$$SSE=\sum(y_i-\hat{y_i})^2$ R平方也有其局限性：R平方随着自变量的增加会变大，R平方和样本量是有关系的。因此，我们要到R平方进行修正。修正的方法：$R^2=1-\frac{(1-R^2)(N-1)}{N-P-1}$p为预测数目N为样本数目]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十七）非线性回归应用]]></title>
    <url>%2F2017-9-13-two%2F</url>
    <content type="text"><![CDATA[直接上一段简单的梯度下降算法代码1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport randomdef genData(numPoints,bias,variance): x = np.zeros(shape=(numPoints,2)) y = np.zeros(shape=(numPoints)) for i in range(0,numPoints): x[i][0]=1 x[i][1]=i y[i]=(i+bias)+random.uniform(0,1)+variance return x,ydef gradientDescent(x,y,theta,alpha,m,numIterations): xTran = np.transpose(x) for i in range(numIterations): hypothesis = np.dot(x,theta) loss = hypothesis-y cost = np.sum(loss**2)/(2*m) gradient=np.dot(xTran,loss)/m theta = theta-alpha*gradient return thetax,y = genData(100, 25, 10)print "x:"print xprint "y:"print ym,n = np.shape(x)n_y = np.shape(y) print("m:"+str(m)+" n:"+str(n)+" n_y:"+str(n_y)) numIterations = 100000alpha = 0.0005theta = np.ones(n)theta= gradientDescent(x, y, theta, alpha, m, numIterations)print(theta) cost由开始的60多下降到最后的3点几最后给出参数theta x数组第一列为1，所以theta[1]代表偏置,theta[2]为斜率]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex符号大全]]></title>
    <url>%2F2017-9-13-one%2F</url>
    <content type="text"><![CDATA[转载自http://blog.csdn.net/garfielder007/article/details/51646604mark一下备用]]></content>
      <tags>
        <tag>LATEX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十六）非线性回归原理]]></title>
    <url>%2F2017-9-12-one%2F</url>
    <content type="text"><![CDATA[概率：1.1 定义 概率(P)robability: 对一件事情发生的可能性的衡量1.2 范围 0 &lt;= P &lt;= 11.3 计算方法：1.3.1 根据个人置信1.3.2 根据历史数据1.3.3 根据模拟数据 1.4 条件概率：$P(A|B)=\frac{P(A\cap B)}{P(B)}$ Logistic Regression (逻辑回归)2.1 例子h(x) &gt; 0.5h(x) &gt; 0.22.2 基本模型测试数据为X(x0，x1，x2···xn)要学习的参数为： Θ(θ0，θ1，θ2，···θn)$Z=\theta_0 x_0+\theta_1 x_1\theta_2 x_2+\cdots \theta_n x_n$向量表示$Z=\Theta^TX$处理二值数据，引入Sigmoid函数时曲线平滑化$g(Z)=\frac{1}{1+e^{-Z}}$预测函数:$h_\theta(X)=g(\Theta^TX)=\frac{1}{1+e^{-\Theta^TX}}$用概率表示:正例(y=1):$h_\theta(X)=P(y=1|X;\Theta)$反例(y=0):$1-h_\theta(X)=P(y=0|X;\Theta)$2.3 Cost函数线性回归:$\sum_{i=1}^m(h_\theta (x^{(i)})-y^{(i)})^2$$h_\theta(x^{(i)})=\theta_0+\theta_1x^{(i)}$找到合适的 θ0，θ1使上式最小Logistic regression:Cost函数:目标：找到合适的 θ0，θ1使上式最小2.4 解法：梯度下降（gradient decent) $\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta),(j=0\cdots n)$更新法则:$\theta_j=\theta_j-\alpha \sum_{i=1}^m(h_\theta (x^{(i)})-y^{(i)})x_j^{(i)},(j=0\cdots n)$学习率同时对所有的θ进行更新重复更新直到收敛]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多电脑间github同步hexo博文]]></title>
    <url>%2F2017-8-31-one%2F</url>
    <content type="text"><![CDATA[远程仓库及分支建立方法参照博文：https://righere.github.io/2016/10/10/install-hexo/完成远程仓库建立这样在另一台没有任何博文资料的电脑上可以同步github数据，先建立SSH连接，确保github账户关联，然后参照一下代码123456git pull origin hexo //先pull完成本地与远端的融合hexo new post " new blog name"git add .//.的话就只增加更新的文章 git commit -m "XX"//此处为github说明git push origin hexohexo d -g //编译发布]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十五）多元回归分析应用]]></title>
    <url>%2F2017-8-12-two%2F</url>
    <content type="text"><![CDATA[例子一家快递公司送货：X1： 运输里程 X2： 运输次数 Y：总运输时间 Driving Assignment X1=Miles Traveled X2=Number of Deliveries Y= Travel Time (Hours)1 100 4 9.32 50 3 4.83 100 4 8.94 100 2 6.55 50 2 4.26 80 2 6.27 75 3 7.48 65 4 6.09 90 3 7.610 90 2 6.1目的，求出b0, b1,…. bp：y_hat=b0＋b１x1+b2x2+ … +bpxp Python代码：12345678910111213141516171819202122from numpy import genfromtxtfrom sklearn import linear_modeldataPath = r"Delivery.csv"deliveryData = genfromtxt(dataPath,delimiter=',')print "data"print deliveryDatax= deliveryData[:,:-1]y = deliveryData[:,-1]print xprint ylr = linear_model.LinearRegression()lr.fit(x, y)print lrprint("coefficients:")print lr.coef_print("intercept:")print lr.intercept_xPredict = [102,6] yPredict = lr.predict(xPredict)print("predict:")print yPredict# 如果需要多个车型分开则增加N维二进制数组输入 然后预测 如：xPredict = [90,2,0,0,1]]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十四）多元回归分析]]></title>
    <url>%2F2017-8-12-one%2F</url>
    <content type="text"><![CDATA[与简单线性回归区别(simple linear regression)多个自变量(x) 多元回归模型y=β0＋β１x1+β2x2+ … +βpxp+ε其中：β0，β１，β2… βp是参数ε是误差值 多元回归方程E(y)=β0＋β１x1+β2x2+ … +βpxp 估计多元回归方程:y_hat=b0＋b１x1+b2x2+ … +bpxp一个样本被用来计算β0，β１，β2… βp的点估计b0, b1, b2,…, bp 估计流程 (与简单线性回归类似） 估计方法使sum of squares最小运算与简单线性回归类似，涉及到线性代数和矩阵代数的运算 例子一家快递公司送货：X1： 运输里程 X2： 运输次数 Y：总运输时间DrivingAssignment Traveled X1=Miles X2=Number of Deliveries Y= Travel Time (Hours)1 100 4 9.32 50 3 4.83 100 4 8.94 100 2 6.55 50 2 4.26 80 2 6.27 75 3 7.48 65 4 6.09 90 3 7.610 90 2 6.1Time = b0+ b1Miles + b2 DeliveriesTime = -0.869 + 0.0611 Miles + 0.923 Deliveries 描述参数含义b0: 平均每多运送一英里，运输时间延长0.0611 小时b1: 平均每多一次运输，运输时间延长 0.923 小时 预测如果一个运输任务是跑102英里，运输6次，预计多少小时？Time = -0.869 +0.0611 102+ 0.923 6= 10.9 (小时） 如果自变量中有分类型变量(categorical data) , 如何处理？英里数 次数 车型 时间100 4 1 9.350 3 0 4.8100 4 1 8.9100 2 2 6.550 2 2 4.280 2 1 6.275 3 1 7.465 4 0 690 3 0 7.6 关于误差的分布误差ε是一个随机变量，均值为0ε的方差对于所有的自变量来说相等所有ε的值是独立的ε满足正态分布，并且通过β0＋β１x1+β2x2+ … +βpxp反映y的期望值]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十三）简单线性回归应用]]></title>
    <url>%2F2017-7-8-two%2F</url>
    <content type="text"><![CDATA[简单线性回归模型举例：汽车卖家做电视广告数量与卖出的汽车数量：1.1 如何练处适合简单线性回归模型的最佳回归线？ min$\sum(y_i-\hat y_i)^2$使sum of squares最小1.1.2 计算$b_1=\frac{\sum(x_i-\overline x)(y_i-\overline y)}{\sum(x_i-\overline x)^2}$$b_0=\overline y-b_1\overline x$分子 = (1-2)(14-20)+(3-2)(24-20)+(2-2)(18-20)+(1-2)(17-20)+(3-2)(27-20)= 6 + 4 + 0 + 3 + 7= 20分母 = （1-2）^2 + (3-2)^2 + (2-2)^2 + (1-2)^2 + (3-2)^2= 1 + 1 + 0 + 1 + 1= 4b1 = 20/4 =5b0 = 20 - 5$\times$2 = 20 - 10 = 101.2 预测：假设有一周广告数量为6，预测的汽车销售量是多少？x_given = 6Y_hat = 5$\times$6 + 10 = 401.3 Python实现：1234567891011121314151617181920import numpy as npdef fitSLR(x, y): n = len(x) dinominator = 0 numerator = 0 for i in range(0, n): numerator += (x[i] - np.mean(x))*(y[i] - np.mean(y)) dinominator += (x[i] - np.mean(x))**2 b1 = numerator/float(dinominator) b0 = np.mean(y)-b1*float(np.mean(x)) return b0, b1def predict(x, b0, b1): return b0 + x*b1x = [1, 3, 2, 1, 3]y = [14, 24, 18, 17, 27] b0, b1 = fitSLR(x, y)print "intercept:", b0, " slope:", b1x_test = 6y_test = predict(6, b0, b1)print "y_test:", y_test]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十二）简单线性回归原理]]></title>
    <url>%2F2017-7-8-one%2F</url>
    <content type="text"><![CDATA[为什么需要统计量？统计量：描述数据特征1.1集中趋势衡量1.1.1均值（平均数，平均值）（mean）$\overline x = \sum_{i=1}^{n}x_i${6, 2, 9, 1, 2}(6 + 2 + 9 + 1 + 2) / 5 = 20 / 5 = 41.1.2中位数 （median）: 将数据中的各个数值按照大小顺序排列，居于中间位置的变量1.1.2.1给数据排序：1， 2， 2， 6， 91.1.2.2找出位置处于中间的变量：2当n为基数的时候：直接取位置处于中间的变量当n为偶数的时候，取中间两个量的平均值1.1.2众数 （mode）：数据中出现次数最多的数 1.21.2.1离散程度衡量1.2.1.1方差（variance)$s^2=\frac{\sum_{i=1}^n(x_i-\overline x)^2}{n-1}${6, 2, 9, 1, 2}(1) (6 - 4)^2 + (2 - 4) ^2 + (9 - 4)^2 + (1 - 4)^2 + (2 - 4)^2= 4 + 4 + 25 + 9 + 4= 46(2) n - 1 = 5 - 1 = 4(3) 46 / 4 = 11.51.2.1.2标准差 (standard deviation)$s=\sqrt{s^2}$s = sqrt(11.5) = 3.39 简单线性回归(Simple Linear Regression)2.1很多做决定过过程通常是根据两个或者多个变量之间的关系2.2回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联2.3被预测的变量叫做：因变量(dependent variable), y, 输出(output)2.4被用来进行预测的变量叫做： 自变量(independent variable), x, 输入(input) 简单线性回归介绍3.1简单线性回归包含一个自变量(x)和一个因变量(y)3.2以上两个变量的关系用一条直线来模拟3.3如果包含两个以上的自变量，则称作多元回归分析(multiple regression) 简单线性回归模型4.1被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型4.2简单线性回归的模型是:$y=\beta_0+\beta_1x+\varepsilon$ 简单线性回归方程E(y) = β0+β1x这个方程对应的图像是一条直线，称作回归线其中，β0是回归线的截距β1是回归线的斜率E(y)是在一个给定x值下y的期望值（均值） 正向线性关系： 负向线性关系： 无关系 估计的简单线性回归方程ŷ=b0+b1x这个方程叫做估计线性方程(estimated regression line)其中，b0是估计线性方程的纵截距b1是估计线性方程的斜率ŷ是在自变量x等于一个给定值的时候，y的估计值 线性回归分析流程： 关于偏差ε的假定11.1是一个随机的变量，均值为011.2ε的方差(variance)对于所有的自变量x是一样的11.3ε的值是独立的11.4ε满足正态分布]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十一）神经网络应用]]></title>
    <url>%2F2017-7-3-three%2F</url>
    <content type="text"><![CDATA[关于非线性转化方程(non-linear transformation function)sigmoid函数(S 曲线)用来作为activation function:1.1双曲函数(tanh)1.2逻辑函数(logistic function) 用类实现一个简单的神经网络算法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import numpy as npdef tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x)*np.tanh(x)def logistic(x): return 1/(1 + np.exp(-x))def logistic_derivative(x): return logistic(x)*(1-logistic(x))class NeuralNetwork: def __init__(self, layers, activation='tanh'): """ :param layers: A list containing the number of units in each layer. Should be at least two values :param activation: The activation function to be used. Can be "logistic" or "tanh" """ if activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv self.weights = [] for i in range(1, len(layers) - 1): self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25) self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25) def fit(self, X, y, learning_rate=0.2, epochs=10000): X = np.atleast_2d(X) temp = np.ones([X.shape[0], X.shape[1]+1]) temp[:, 0:-1] = X # adding the bias unit to the input layer X = temp y = np.array(y) for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] for l in range(len(self.weights)): #going forward network, for each layer a.append(self.activation(np.dot(a[l], self.weights[l]))) #Computer the node value for each layer (O_i) using activation function error = y[i] - a[-1] #Computer the error at the top layer deltas = [error * self.activation_deriv(a[-1])] #For output layer, Err calculation (delta is updated error) #Staring backprobagation for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer #Compute the updated error (i,e, deltas) for each node going from top layer to input layer deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l])) deltas.reverse() for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta) def predict(self, x): x = np.array(x) temp = np.ones(x.shape[0]+1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return a 简单非线性关系数据集测试(XOR):X: Y0 0 00 1 11 0 11 1 0 12345678from NeuralNetwork import NeuralNetworkimport numpy as npnn = NeuralNetwork([2,2,1], 'tanh') X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([0, 1, 1, 0]) nn.fit(X, y) for i in [[0, 0], [0, 1], [1, 0], [1,1]]: print(i, nn.predict(i)) 手写数字识别：每个图片8x8识别数字：0,1,2,3,4,5,6,7,8,9 1234567891011121314151617181920212223import numpy as np from sklearn.datasets import load_digits from sklearn.metrics import confusion_matrix, classification_report from sklearn.preprocessing import LabelBinarizer from NeuralNetwork import NeuralNetworkfrom sklearn.cross_validation import train_test_splitdigits = load_digits() X = digits.data y = digits.target X -= X.min() # normalize the values to bring them into the range 0-1 X /= X.max()nn = NeuralNetwork([64,100,10],'logistic') X_train, X_test, y_train, y_test = train_test_split(X, y) labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test)print "start fitting"nn.fit(X_train,labels_train,epochs=3000) predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) print confusion_matrix(y_test,predictions) print classification_report(y_test,predictions)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决hexo数学公式编辑不正确的问题]]></title>
    <url>%2F2017-7-3-two%2F</url>
    <content type="text"><![CDATA[hexo在编译数学公式时有时会将_渲染错误造成｛｝内多个下标无法正确识别的情况，本文给出解决办法.找到marked.js文件 去掉对_渲染的部分，修改之后如下图]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（十）神经网络原理]]></title>
    <url>%2F2017-7-3-one%2F</url>
    <content type="text"><![CDATA[背景:1.1以人脑中的神经网络为启发，历史上出现过很多不同版本1.2最著名的算法是1980年的 backpropagation 多层向前神经网络(Multilayer Feed-Forward Neural Network)2.1Backpropagation被使用在多层向前神经网络上2.2多层向前神经网络由以下部分组成： 输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)2.3每层由单元(units)组成2.4输入层(input layer)是由训练集的实例特征向量传入2.5经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入2.6隐藏层的个数可以是任意的，输入层有一层，输出层有一层2.7每个单元(unit)也可以被称作神经结点，根据生物学来源定义2.8以上成为2层的神经网络（输入层不算）2.8一层中加权的求和，然后根据非线性方程转化输出2.9作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程 设计神经网络结构3.1使用神经网络训练数据之前，必须确定神经网络的层数，以及每层单元的个数3.2特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）3.3离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值比如：特征值A可能取三个值（a0, a1, a2), 可以使用3个输入单元来代表A。如果A=a0, 那么代表a0的单元值就取1, 其他取0；如果A=a1, 那么代表a1de单元值就取1，其他取0，以此类推3.4神经网络即可以用来做分类(classification）问题，也可以解决回归(regression)问题3.4.1对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类）如果多余2类，每一个类别用一个输出单元表示所以输入层的单元数量通常等于类别的数量3.4.2没有明确的规则来设计最好有多少个隐藏层3.4.2.1根据实验测试和误差，以及准确度来实验并改进 交叉验证方法(Cross-Validation)K-fold cross validation Backpropagation算法5.1通过迭代性的来处理训练集中的实例5.2对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间5.3反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差(error)来更新每个连接的权重(weight)5.4算法详细介绍输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络输入：一个训练好的神经网络(a trained neural network)5.4.1初始化权重(weights)和偏向(bias): 随机初始化在-1到1之间，或者-0.5到0.5之间，每个单元有一个偏向5.4.2对于每一个训练实例X，执行以下步骤：5.4.2.1:由输入层向前传送$I_j=\sum_i w_{ij}O_i+\theta j$$O_j=\frac{1}{1+e^{I_j}}$5.4.2.2根据误差(error)反向传送对于输出层：$Err_j=O_j(1-O_j)(T_j-O_j)$对于隐藏层：$Err_j=O_j(1-O_j)\sum{Err_kw_{jk}}$$\bigtriangleup w_{ij}=(l) Err_jO_i$权重更新：$w_{ij}=w_{ij}+\bigtriangleup w_{ij}$偏向更新$\bigtriangleup \theta_j=(l) Err_j$$\theta_j=\theta_j+\bigtriangleup \theta_j$5.4.3终止条件5.4.3.1权重的更新低于某个阈值5.4.3.2预测的错误率低于某个阈值5.4.3.3达到预设一定的循环次数 Backpropagation 算法举例对于输出层：$Err_j=O_j(1-O_j)(T_j-O_j)$对于隐藏层：$Err_j=O_j(1-O_j)\sum{Err_kw_{jk}}$$\bigtriangleup w_{ij}=(l) Err_jO_i$权重更新：$w_{ij}=w_{ij}+\bigtriangleup w_{ij}$偏向更新$\bigtriangleup \theta_j=(l) Err_j$$\theta_j=\theta_j+\bigtriangleup \theta_j$]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（九）利用SVM进行人脸识别]]></title>
    <url>%2F2017-7-1-one%2F</url>
    <content type="text"><![CDATA[利用SVM进行人脸识别代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394from __future__ import print_functionfrom time import timeimport loggingimport matplotlib.pyplot as pltfrom sklearn.cross_validation import train_test_splitfrom sklearn.datasets import fetch_lfw_peoplefrom sklearn.grid_search import GridSearchCVfrom sklearn.metrics import classification_reportfrom sklearn.metrics import confusion_matrixfrom sklearn.decomposition import RandomizedPCAfrom sklearn.svm import SVCprint(__doc__)# Display progress logs on stdoutlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')################################################################################ Download the data, if not already on disk and load it as numpy arrayslfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)# introspect the images arrays to find the shapes (for plotting)n_samples, h, w = lfw_people.images.shape# for machine learning we use the 2 data directly (as relative pixel# positions info is ignored by this model)X = lfw_people.datan_features = X.shape[1]# the label to predict is the id of the persony = lfw_people.targettarget_names = lfw_people.target_namesn_classes = target_names.shape[0]print("Total dataset size:")print("n_samples: %d" % n_samples)print("n_features: %d" % n_features)print("n_classes: %d" % n_classes)################################################################################ Split into a training set and a test set using a stratified k fold# split into a training and testing setX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25)################################################################################ Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled# dataset): unsupervised feature extraction / dimensionality reductionn_components = 150print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))t0 = time()pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)print("done in %0.3fs" % (time() - t0))eigenfaces = pca.components_.reshape((n_components, h, w))print("Projecting the input data on the eigenfaces orthonormal basis")t0 = time()X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print("done in %0.3fs" % (time() - t0))################################################################################ Train a SVM classification modelprint("Fitting the classifier to the training set")t0 = time()param_grid = &#123;'C': [1e3, 5e3, 1e4, 5e4, 1e5], 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], &#125;clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)clf = clf.fit(X_train_pca, y_train)print("done in %0.3fs" % (time() - t0))print("Best estimator found by grid search:")print(clf.best_estimator_)################################################################################ Quantitative evaluation of the model quality on the test setprint("Predicting people's names on the test set")t0 = time()y_pred = clf.predict(X_test_pca)print("done in %0.3fs" % (time() - t0))print(classification_report(y_test, y_pred, target_names=target_names))print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))################################################################################ Qualitative evaluation of the predictions using matplotlibdef plot_gallery(images, titles, h, w, n_row=3, n_col=4): """Helper function to plot a gallery of portraits""" plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35) for i in range(n_row * n_col): plt.subplot(n_row, n_col, i + 1) plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) plt.title(titles[i], size=12) plt.xticks(()) plt.yticks(())# plot the result of the prediction on a portion of the test setdef title(y_pred, y_test, target_names, i): pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1] true_name = target_names[y_test[i]].rsplit(' ', 1)[-1] return 'predicted: %s\ntrue: %s' % (pred_name, true_name)prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]plot_gallery(X_test, prediction_titles, h, w)# plot the gallery of the most significative eigenfaceseigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]plot_gallery(eigenfaces, eigenface_titles, h, w)plt.show()]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（八）支持向量机（SVM）原理下]]></title>
    <url>%2F2017-6-30-three%2F</url>
    <content type="text"><![CDATA[SVM算法特性：1.1训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting1.2SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。1.3一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。 线性不可分的情况 （linearly inseparable case)2.1数据集在空间中对应的向量不可被一个超平面区分开2.2两个步骤来解决：2.2.1利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中2.2.2在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理2.2.3视觉化演示 https://www.youtube.com/watch?v=3liCbRZPrZA2.3如何利用非线性映射把原始数据转化到高维中？2.3.1 例子：3维输入向量：$X=${$x_1,x_2,x_3$)转化到6维空间 Z 中去：$\phi_1(X)=x1,\phi_2(X)=x2,\phi_3(X)=x3,\phi_4(X)=x1^2,\phi_5(X)=x1x2,\phi_6(X)=x1x3$新的决策超平面： $d(Z)=WZ+b$ 其中W和Z是向量，这个超平面是线性的解出W和b之后，并且带入回原方程：d(Z)=w1x1+w2x2+w3x3+$w4x1^2$+w5x1x2+w3x1x3+b =w1z1+w2z2+w3z3+w4z4+w5z5+w6z6+b2.3.2思考问题：2.3.2.1如何选择合理的非线性转化把数据转到高纬度中？2.3.2.2如何解决计算内积时算法复杂度非常高的问题？2.3.3使用核方法（kernel trick) 核方法（kernel trick)3.1动机在线性SVM中转化为最优化问题时求解的公式计算都是以内积(dot product)的形式出现的$\phi(X_i)\phi(X_j)$，其中$\phi(x)$是把训练集中的向量点转化到高维的非线性映射函数，因为内积的算法复杂度非常大，所以我们利用核函数来取代计算非线性映射函数的内积3.1以下核函数和非线性映射函数的内积等同$K(X_i,X_j)=\phi(X_i)\phi(X_j)$3.2 常用的核函数(kernel functions)h度多项式核函数(polynomial kernel of degree h)： $K(X_i,X_j)=(X_iX_j+1)^n$高斯径向基核函数(Gaussian radial basis function kernel): $K(X_i,X_j)=e^{-||X_i-X_j||^2/2\sigma^2}$S型核函数(Sigmoid function kernel): $K(X_i,X_j)=tanh(\alpha X_iX_j-\delta)$如何选择使用哪个kernel？根据先验知识，比如图像分类，通常使用RBF，文字不使用RBF尝试不同的kernel，根据结果准确度而定3.3 核函数举例:假设定义两个向量： x = (x1, x2, x3); y = (y1, y2, y3)定义方程：f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3)K(x, y ) = (xy)^2假设x = (1, 2, 3); y = (4, 5, 6).f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)f(y) = (16, 20, 24, 20, 25, 36, 24, 30, 36) = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024同样的结果，使用kernel方法计算容易很多 SVM扩展可解决多个类别分类问题对于每个类，有一个当前类和其他类的二类分类器（one-vs-rest)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（七）支持向量机（SVM）应用上]]></title>
    <url>%2F2017-6-30-two%2F</url>
    <content type="text"><![CDATA[1 sklearn简单例子123456789101112from sklearn import svmX = [[2, 0], [1, 1], [2,3]]y = [0, 0, 1]clf = svm.SVC(kernel = 'linear')clf.fit(X, y) print clf# get support vectorsprint clf.support_vectors_# get indices of support vectorsprint clf.support_ # get number of support vectors for each classprint clf.n_support_ 2 sklearn画出决定界限123456789101112131415161718192021222324252627282930313233343536373839404142print(__doc__)import numpy as npimport pylab as plfrom sklearn import svm# we create 40 separable pointsnp.random.seed(0)X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]Y = [0] * 20 + [1] * 20# fit the modelclf = svm.SVC(kernel='linear')clf.fit(X, Y)# get the separating hyperplanew = clf.coef_[0]a = -w[0] / w[1]xx = np.linspace(-5, 5)yy = a * xx - (clf.intercept_[0]) / w[1]# plot the parallels to the separating hyperplane that pass through the# support vectorsb = clf.support_vectors_[0]yy_down = a * xx + (b[1] - a * b[0])b = clf.support_vectors_[-1]yy_up = a * xx + (b[1] - a * b[0])print "w: ", wprint "a: ", a# print " xx: ", xx# print " yy: ", yyprint "support_vectors_: ", clf.support_vectors_print "clf.coef_: ", clf.coef_# In scikit-learn coef_ attribute holds the vectors of the separating hyperplanes for linear models. It has shape (n_classes, n_features) if n_classes &gt; 1 (multi-class one-vs-all) and (1, n_features) for binary classification.# # In this toy binary classification example, n_features == 2, hence w = coef_[0] is the vector orthogonal to the hyperplane (the hyperplane is fully defined by it + the intercept).# # To plot this hyperplane in the 2D case (any hyperplane of a 2D plane is a 1D line), we want to find a f as in y = f(x) = a.x + b. In this case a is the slope of the line and can be computed by a = -w[0] / w[1].# plot the line, the points, and the nearest vectors to the planepl.plot(xx, yy, 'k-')pl.plot(xx, yy_down, 'k--')pl.plot(xx, yy_up, 'k--')pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)pl.axis('tight')pl.show()]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（六）支持向量机（SVM）原理上]]></title>
    <url>%2F2017-6-30-one%2F</url>
    <content type="text"><![CDATA[背景：1.1 最早是由 Vladimir N. Vapnik 和 Alexey Ya. Chervonenkis 在1963年提出1.2 目前的版本(soft margin)是由Corinna Cortes 和 Vapnik在1993年提出，并在1995年发表1.3 深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法 机器学习的一般框架：训练集 =&gt; 提取特征向量 =&gt; 结合一定的算法（分类器：比如决策树，KNN）=&gt;得到结果 介绍：3.1例子：两类？哪条线最好？ 3.2SVM寻找区分两类的超平面（hyper plane), 使边际(margin)最大总共可以有多少个可能的超平面？无数条如何选取使边际(margin)最大的超平面 (Max Margin Hyperplane)？超平面到一侧最近点的距离等于到另一侧最近点的距离，两侧的两个超平面平行3.3线性可区分(linear separable) 和 线性不可区分 （linear inseparable)上图皆为线性不可区分 定义与公式建立超平面可以定义为：$W\dot X+b=0$W: weight vectot,$W$={$w_1,w_2,\cdots,w_n$} , n 是特征值的个数X: 训练实例b: bias4.1假设2维特征向量：X = (x1, X2)把 b 想象为额外的 wight超平面方程变为： $w_0+w_1x_1+w_2x_2=0$所有超平面右上方的点满足：$w_0+w_1x_1+w_2x_2&gt;0$所有超平面左下方的点满足： $w_0+w_1x_1+w_2x_2&lt;0$调整weight，使超平面定义边际的两边：$H1:w_0+w_1x_1+w_2x_2\ge 1 \text{for} y_i=+1$$H2:w_0+w_1x_1+w_2x_2\le -1 \text{for} y_i=-1$综合以上两式，得到：$y_i(w_0+w_1x_1+w_2x_2)\ge 1,\forall i$（1）所有坐落在边际的两边的的超平面上的被称作”支持向量(support vectors)”分界的超平面和H1或H2上任意一点的距离为 $\frac{1}{||w||}$ (i.e.: 其中||W||是向量的范数(norm))所以，最大边际距离为：$\frac{2}{||w||}$ 求解5.1SVM如何找出最大边际的超平面呢(MMH)？利用一些数学推倒，以上公式 （1）可变为有限制的凸优化问题(convex quadratic optimization)利用 Karush-Kuhn-Tucker (KKT)条件和拉格朗日公式，可以推出MMH可以被表示为以下“决定边界 (decision boundary)”$d(X^T)=\sum{y_i\alpha_iX_iX^T}+b_0$其中，$y_i$ 是支持向量点$X_i$（support vector)的类别标记（class label)$X^T$是要测试的实例$\alpha _i$ 和 $b_0$ 都是单一数值型参数，由以上提到的最有算法得出$l$ 是支持向量点的个数5.2对于任何测试（要归类的）实例，带入以上公式，得出的符号是正还是负决定 例子：]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（五）最临近分类算法应用（KNN）]]></title>
    <url>%2F2017-6-28-two%2F</url>
    <content type="text"><![CDATA[数据集介绍：虹膜150个实例萼片长度，萼片宽度，花瓣长度，花瓣宽度(sepal length, sepal width, petal length and petal width）类别：Iris setosa, Iris versicolor, Iris virginica. 利用Python的机器学习库sklearn: SkLearnExample.py 12345678from sklearn import neighborsfrom sklearn import datasetsknn = neighbors.KNeighborsClassifier()iris = datasets.load_iris()print irisknn.fit(iris.data, iris.target)predictedLabel = knn.predict([[0.1, 0.2, 0.3, 0.4]])print predictedLabel 自编代码KNN 实现Implementation: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# Example of kNN implemented from Scratch in Pythonimport csvimport randomimport mathimport operatordef loadDataset(filename, split, trainingSet=[] , testSet=[]): with open(filename, 'rb') as csvfile: lines = csv.reader(csvfile) dataset = list(lines) for x in range(len(dataset)-1): for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random() &lt; split: trainingSet.append(dataset[x]) else: testSet.append(dataset[x])def euclideanDistance(instance1, instance2, length): distance = 0 for x in range(length): distance += pow((instance1[x] - instance2[x]), 2) return math.sqrt(distance)def getNeighbors(trainingSet, testInstance, k): distances = [] length = len(testInstance)-1 for x in range(len(trainingSet)): dist = euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key=operator.itemgetter(1)) neighbors = [] for x in range(k): neighbors.append(distances[x][0]) return neighborsdef getResponse(neighbors): classVotes = &#123;&#125; for x in range(len(neighbors)): response = neighbors[x][-1] if response in classVotes: classVotes[response] += 1 else: classVotes[response] = 1 sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedVotes[0][0]def getAccuracy(testSet, predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: correct += 1 return (correct/float(len(testSet))) * 100.0 def main(): # prepare data trainingSet=[] testSet=[] split = 0.67 loadDataset(r'D:\MaiziEdu\DeepLearningBasics_MachineLearning\Datasets\iris.data.txt', split, trainingSet, testSet) print 'Train set: ' + repr(len(trainingSet)) print 'Test set: ' + repr(len(testSet)) # generate predictions predictions=[] k = 3 for x in range(len(testSet)): neighbors = getNeighbors(trainingSet, testSet[x], k) result = getResponse(neighbors) predictions.append(result) print('&gt; predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet, predictions) print('Accuracy: ' + repr(accuracy) + '%')]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（四）最临近分类算法原理（KNN）]]></title>
    <url>%2F2017-6-28-one%2F</url>
    <content type="text"><![CDATA[综述1.1Cover和Hart在1968年提出了最初的邻近算法1.2分类(classification)算法1.3输入基于实例的学习(instance-based learning), 懒惰学习(lazy learning) 例子：未知电影属于什么类型？ 算法详述3.1步骤：为了判断未知实例的类别，以所有已知类别的实例作为参照选择参数K计算未知实例与所有已知实例的距离选择最近K个已知实例根据少数服从多数的投票法则(majority-voting)，让未知实例归类为K个最邻近样本中最多数的类别3.2细节:关于K关于距离的衡量方法:3.2.1Euclidean Distance 定义其他距离衡量：余弦值（cos）, 相关度 （correlation）, 曼哈顿距离 （Manhattan distance）3.3举例 算法优缺点：4.1算法优点简单易于理解容易实现通过对K的选择可具备丢噪音数据的健壮性4.2算法缺点 需要大量空间储存所有已知实例算法复杂度高（需要比较所有已知实例与要分类的实例）当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例实际并木接近目标样本 改进版本考虑距离，根据距离加上权重比如: 1/d (d: 距离）]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（七）END]]></title>
    <url>%2F2017-6-27-four%2F</url>
    <content type="text"><![CDATA[面向对象编程Python支持面向对象编程类(class)：现实世界中一些事物的封装 （如：学生）类：属性 （如：名字，成绩）类对象实例对象引用：通过引用对类的属性和方法进行操作实例化：创建一个类的具体实例对象 （如：学生张三） 12345678910111213141516#Python OO exampleclass Student: def __init__(self, name, grade): self.name = name self.grade = grade def introduce(self): print("hi! I'm " + self.name) print("my grade is: " + str(self.grade)) def improve(self, amount): self.grade = self.grade + amountjim = Student("jim", 86)jim.introduce()jim.improve(10)jim.introduce() 装饰器(decorator) 12345678910111213141516171819202122232425262728293031323334353637# def add_candles(cake_func):# def insert_candles():# return cake_func() + " candles"# return insert_candles# # def make_cake():# return "cake"# # gift_func = add_candles(make_cake)# # print(make_cake())# print(gift_func())# def add_candles(cake_func):# def insert_candles():# return cake_func() + " candles"# return insert_candles# # def make_cake():# return "cake"# # make_cake = add_candles(make_cake)# # print(make_cake())# # print(gift_func)# def add_candles(cake_func):# def insert_candles():# return cake_func() + " and candles"# return insert_candles# # @add_candles# def make_cake():# return "cake"# # # make_cake = add_candles(make_cake)# # print(make_cake())# # print(gift_func) GUI： Graphical User Interface tkinter: GUI library for Python GUI Example12345678910111213141516# -*- coding: utf-8 -*-from tkinter import *import tkinter.simpledialog as dlimport tkinter.messagebox as mb#tkinter GUI Input Output Example#设置GUIroot = Tk()w = Label(root, text = "Label Title")w.pack() #欢迎消息mb.showinfo("Welcome", "Welcome Message")guess = dl.askinteger("Number", "Enter a number") output = 'This is output message'mb.showinfo("Output: ", output) 6.猜数字游戏代码123456789101112131415161718192021222324252627282930# #设置GUI# root = Tk()# w = Label(root, text = "Guess Number Game")# w.pack()# # #欢迎消息# mb.showinfo("Welcome", "Welcome to Guess Number Game")# # # #处理信息# number = 59# # while True:# #让用户输入信息# guess = dl.askinteger("Number", "What's your guess?")# # if guess == number:# # New block starts here# output = 'Bingo! you guessed it right, but you do not win any prizes!'# mb.showinfo("Hint: ", output)# break# # New block ends here# elif guess &lt; number:# output = 'No, the number is a higer than that'# mb.showinfo("Hint: ", output)# else:# output = 'No, the number is a lower than that'# mb.showinfo("Hint: ", output)# # print('Done') 7.网页编程介绍7.1. 下载并安装python 2.7 32 bit7.2. 下载并安装easy_install windows installer (python 2.7 32bit)7.3. 安装 lpthw.web windows 命令行： C:\Python27\Scripts\easy_install lpthw.web7.4. 创建目录 C:\Users\plg519\Maizi\TeachingPython\gotoweb\bin7.5. 目录下创建 app.py:1234567891011import weburls = ( '/', 'index')app = web.application(urls, globals())class index: def GET(self): greeting = "Hello World" return greetingif __name__ == "__main__": app.run() 7.6. Windows cmd 运行：cd C:\Users\plg519\Maizi\TeachingPython\gotoweb\binC:\python27\python27 app.py7.7. 打开浏览器：localhost：8080]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（六）]]></title>
    <url>%2F2017-6-27-three%2F</url>
    <content type="text"><![CDATA[1.输入格式： input()2.输出格式：print(),str(), format1234str_1 = input("Enter a string: ")str_2 = input("Enter another string: ")print("str_1 is: " + str_1 + ". str_2 is :" + str_2)print("str_1 is &#123;&#125; + str_2 is &#123;&#125;".format(str_1, str_2)) 写出文件 读入文件 1234567891011121314151617181920some_sentences = '''\I love learning pythonbecause python is funand also easy to use'''#Open for 'w'irtingf = open('sentences.txt', 'w')#Write text to Filef.write(some_sentences)f.close()#If not specifying mode, 'r'ead mode is defaultf = open('sentences.txt')while True: line = f.readline() #Zero length means End Of File if len(line) == 0: break print(line)# close the Filef.close() 5.Python有两种错误类型：5.1. 语法错误(Syntax Errors)5.2. 异常（Exceptions)首先，try语句下的（try和except之间的代码）被执行如果没有出现异常，except语句将被忽略如果try语句之间出现了异常，try之下异常之后的代码被忽略，直接跳跃到except语句如果异常出现，但并不属于except中定义的异常类型，程序将执行外围一层的try语句，如果异常没有被处理，将产生unhandled exception的错误处理异常（Handling Exceptions)Exception doc: https://docs.python.org/3.4/library/exceptions.html1234567891011121314#Example of Syntax errors# while True print("Hello World!")#Examples of exceptions# print(8/0)# print(hello * 4)# num = 6# print("Hello World " + num )#Handling exceptions# while True:# try:# x = int(input("Please enter a number"))# break# except ValueError:# print("Not valid input, try again...")]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（三）决策树算法代码与应用]]></title>
    <url>%2F2017-6-27-two%2F</url>
    <content type="text"><![CDATA[Python Python机器学习的库：scikit-learn2.1： 特性：简单高效的数据挖掘和机器学习分析对所有用户开放，根据不同需求高度可重用性基于Numpy, SciPy和matplotlib开源，商用级别：获得 BSD许可2.2 覆盖问题领域：分类（classification), 回归（regression), 聚类（clustering), 降维(dimensionality reduction)模型选择(model selection), 预处理(preprocessing) 使用用scikit-learn安装scikit-learn: pip, easy_install, windows installer安装必要package：numpy， SciPy和matplotlib， 可使用Anaconda (包含numpy, scipy等科学计算常用package）安装注意问题：Python解释器版本（2.7 or 3.4？）, 32-bit or 64-bit系统 例子：文档： http://scikit-learn.org/stable/modules/tree.htmlDecesionTree 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445from sklearn.feature_extraction import DictVectorizerimport csvfrom sklearn import treefrom sklearn import preprocessingfrom sklearn.externals.six import StringIO#Read in the csv file and put features into list of dict and list of class labelallElectronicsData = open(r'C:\Users\Administrator\workspace\Machinelearning\DecesionTree\data.csv', 'rb')reader = csv.reader(allElectronicsData)headers = reader.next()print(headers)featureList = []labelList = []for row in reader: labelList.append(row[len(row)-1]) rowDict = &#123;&#125; #Dictionary for i in range(1, len(row)-1): rowDict[headers[i]] = row[i] featureList.append(rowDict)print(labelList)#Vetorize featuresvec = DictVectorizer()dummyX = vec.fit_transform(featureList) .toarray()print("dummyX: " + str(dummyX))print(vec.get_feature_names())#vectorize class labelslb = preprocessing.LabelBinarizer()dummyY = lb.fit_transform(labelList)print("dummyY: " + str(dummyY))#Using decision tree for classification#clf = tree.DecisionTreeClassifier()clf = tree.DecisionTreeClassifier(criterion='entropy')clf = clf.fit(dummyX, dummyY)print("clf: " + str(clf))# Visualize modelwith open("DataAfterDecesionTree.dot", 'w') as f: f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)#Predict oneRowX = dummyX[0, :]print("oneRowX: " + str(oneRowX))newRowX = oneRowXnewRowX[0] = 1newRowX[2] = 0print("newRowX: " + str(newRowX))predictedY = clf.predict(newRowX)print("predictedY: " + str(predictedY)) 安装 Graphviz： http://www.graphviz.org/可以将决策树图形化直接输出决策树图形配置环境变量转化dot文件至pdf可视化决策树：dot -Tpdf iris.dot -o outpu.pdf]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二）决策树算法原理]]></title>
    <url>%2F2017-6-27-one%2F</url>
    <content type="text"><![CDATA[机器学习中分类和预测算法的评估：准确率速度强壮行可规模性可解释性 什么是决策树/判定树（decision tree)? 判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。 机器学习中分类方法中的一个重要算法 构造决策树的基本算法3.1 熵（entropy）概念：信息和抽象，如何度量？1948年，香农提出了 ”信息熵(entropy)“的概念一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==&gt;信息量的度量就等于不确定性的多少例子：猜世界杯冠军，假如一无所知，猜多少次？每个队夺冠的几率不是相等的比特(bit)来衡量信息的多少变量的不确定性越大，熵也就越大3.1 决策树归纳算法 （ID3）1970-1980， J.Ross. Quinlan, ID3算法选择属性判断结点信息获取量(Information Gain)：Gain(A) = Info(D) - Infor_A(D)通过A来作为节点分类获取了多少信息类似，Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048所以，选择age作为第一个根节点重复。。。算法：树以代表训练样本的单个结点开始（步骤1）。如果样本都在同一个类，则该结点成为树叶，并用该类标号（步骤2 和3）。否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性（步骤6）。该属性成为该结点的“测试”或“判定”属性（步骤7）。在算法的该版本中，所有的属性都是分类的，即离散值。连续属性必须离散化。对测试属性的每个已知的值，创建一个分枝，并据此划分样本（步骤8-10）。算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，就不必该结点的任何后代上考虑它（步骤13）。递归划分步骤仅当下列条件之一成立停止：(a) 给定结点的所有样本属于同一类（步骤2 和3）。(b) 没有剩余属性可以用来进一步划分样本（步骤4）。在此情况下，使用多数表决（步骤5）。这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结点样本的类分布。(c) 分枝test_attribute = a i 没有样本（步骤11）。在这种情况下，以 samples 中的多数类创建一个树叶（步骤12） 3.1 其他算法：C4.5: QuinlanClassification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)共同点：都是贪心算法，自上而下(Top-down approach)区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain)3.2 如何处理连续性变量的属性？ 树剪枝叶 （避免overfitting) 4.1 先剪枝 4.2 后剪枝 决策树的优点： 直观，便于理解，小规模数据集有效 决策树的缺点： 处理连续变量不好 类别较多时，错误增加的比较快 可规模性一般]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解百度云限速]]></title>
    <url>%2F2017-6-26-two%2F</url>
    <content type="text"><![CDATA[致敬制作PCS命令行来处理百度云限速的大神：github帐号@GangZhuo完美的解决了坑爹Baidu限速和拉黑的限制，记录操作流程和简单命令总的来说与Linux操作命令相同 在之前加上PCS即可。Windows：1、命令行版本：下载这个https://github.com/GangZhuo/BaiduPCS/releases/download/0.2.5/pcs-win32-0.2.5-db684dc.zip然后复制到System32里面2、检查是否安装成功：终端（windows命令行版本则打开cmd输入pcs，GUI版本双击打开）输入pcs，查看是否像我的一样，如果是，则安装成功。3、这个时候先设置一下线程数量，这里给一个参考的设置线程数：＜10m：5线程＜50m：10线程＜100m：25线程＜500m：40线程＜1G：70-80线程＞2G：100线程（最多也是只能设置100线程）设置线程命令：pcs set –max_thread=你要设置的线程数例如我现在要设置100线程，则输入pcs set –max_thread=100下载文件：用pcs cd切换到你百度云中要下载的文件目录（例如说“我的应用数据”就是/apps）然后使用pcs download &lt;文件名&gt; &lt;你要保存到的目录加文件名&gt;例如说我要下载“Parrot-full-3.3_amd64.iso”，保存到/home/redapple/Downloads就输入pcs download Parrot-full-3.3_amd64.iso /home/redapple/Downloads/Parrot-full-3.3_amd64.iso注意：如果你要下载的文件带有空格（如1 2 3 ），就要打上引号，就想这样pcs download “1 2 3” “/home/redapple/Downloads/1 2 3”上传文件命令是pcs upload &lt;你要上传的文件目录&gt; &lt;你要上传到云端的目录&gt;]]></content>
      <tags>
        <tag>BaiduYun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（五）if for range 等循环语句]]></title>
    <url>%2F2017-6-26-one%2F</url>
    <content type="text"><![CDATA[1. if 语句if condition: do somethingelif other_condition: do somethingother code 成人的BMI数值 过轻 低于18.5 正常 18.5-23.9 过重 24-27 肥胖 28-32 输入身高体重 给出BMI数值BMI指数例程 2. break终止循环打印倒三角形 3. continue提前结束本次循环猜数字游戏猜数字游戏 4. while语句while condition: do somethingother code打印九九乘法表 5. for 语句12345678910111213141516171819202122232425262728293031323334353637383940414243# #if statement example# # number = 59# guess = int(input('Enter an integer : '))# # if guess == number:# # New block starts here# print('Bingo! you guessed it right.')# print('(but you do not win any prizes!)')# # New block ends here# elif guess &lt; number:# # Another block# print('No, the number is higher than that')# # You can do whatever you want in a block ...# else:# print('No, the number is a lower than that')# # you must have guessed &gt; number to reach here# # print('Done')# # This last statement is always executed,# # after the if statement is executed.#the for loop example# for i in range(1, 10):# print(i)# else:# print('The for loop is over')# # # a_list = [1, 3, 5, 7, 9]# for i in a_list:# print(i)# # a_tuple = (1, 3, 5, 7, 9)# for i in a_tuple:# print(i)# # a_dict = &#123;'Tom':'111', 'Jerry':'222', 'Cathy':'333'&#125;# for ele in a_dict:# print(ele)# print(a_dict[ele])# # for key, elem in a_dict.items():# print(key, elem) 6. range语句1234567891011121314151617181920212223242526272829303132333435363738394041424344#while example# number = 59# guess_flag = False# # # while guess_flag == False:# guess = int(input('Enter an integer : '))# if guess == number:# # New block starts here# guess_flag = True# # # New block ends here# elif guess &lt; number:# # Another block# print('No, the number is higher than that, keep guessing')# # You can do whatever you want in a block ...# else:# print('No, the number is a lower than that, keep guessing')# # you must have guessed &gt; number to reach here# # print('Bingo! you guessed it right.')# print('(but you do not win any prizes!)') # print('Done')#For example number = 59num_chances = 3print("you have only 3 chances to guess")for i in range(1, num_chances + 1): print("chance " + str(i)) guess = int(input('Enter an integer : ')) if guess == number: # New block starts here print('Bingo! you guessed it right.') print('(but you do not win any prizes!)') break # New block ends here elif guess &lt; number: # Another block print('No, the number is higher than that, keep guessing, you have ' + str(num_chances - i) + ' chances left') # You can do whatever you want in a block ... else: print('No, the number is lower than that, keep guessing, you have ' + str(num_chances - i) + ' chances left') # you must have guessed &gt; number to reach hereprint('Done') 7. pass12345678910111213141516#break &amp; continue example#continue and pass difference# a_list = [0, 1, 2]# # print("using continue:")# for i in a_list:# if not i:# continue# print(i)# # print("using pass:") # for i in a_list:# if not i:# pass# print(i)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（四）函数]]></title>
    <url>%2F2017-6-23-three%2F</url>
    <content type="text"><![CDATA[函数：程序中可重复使用的程序段给一段程程序起一个名字，用这个名字来执行一段程序，反复使用 （调用函数）用关键字 ‘def’ 来定义，identifier(参数)identifier参数listreturn statement局部变量 vs 全局变量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#-*- coding: utf-8 -*-#没有参数和返回的函数# def say_hi():# print(" hi!")# # say_hi()# say_hi()# # #有参数，无返回值# def print_sum_two(a, b):# c = a + b# print(c)# # print_sum_two(3, 6) # def hello_some(str):# print("hello " + str + "!")# # hello_some("China")# hello_some("Python") #有参数，有返回值# def repeat_str(str, times):# repeated_strs = str * times# return repeated_strs# # # repeated_strings = repeat_str("Happy Birthday!", 4)# print(repeated_strings) #全局变量与局部 变量# x = 60# # def foo(x):# print("x is: " + str(x))# x = 3# print("change local x to " + str(x))# # foo(x)# print('x is still', str(x))x = 60def foo(): global x print("x is: " + str(x)) x = 3 print("change local x to " + str(x))foo()print('value of x is' , str(x)) 默认参数关键字参数VarArgs参数123456789101112131415161718192021222324#-*- coding: utf-8 -*-# 默认参数def repeat_str(s, times = 1): repeated_strs = s * times return repeated_strsrepeated_strings = repeat_str("Happy Birthday!")print(repeated_strings)repeated_strings_2 = repeat_str("Happy Birthday!" , 4)print(repeated_strings_2)#不能在有默认参数后面跟随没有默认参数#f(a, b =2)合法#f(a = 2, b)非法#关键字参数: 调用函数时，选择性的传入部分参数def func(a, b = 4, c = 8): print('a is', a, 'and b is', b, 'and c is', c)func(13, 17)func(125, c = 24)func(c = 40, a = 80)#VarArgs参数def print_paras(fpara, *nums, **words): print("fpara: " + str(fpara)) print("nums: " + str(nums)) print("words: " + str(words)) print_paras("hello", 1, 3, 5, 7, word = "python", anohter_word = "java")]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（一）]]></title>
    <url>%2F2017-6-23-two%2F</url>
    <content type="text"><![CDATA[基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归 概念学习：人类学习概念：鸟，车，计算机定义：概念学习是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数 例子：学习 “享受运动” 这一概念：小明进行水上运动，是否享受运动取决于很多因素样例 天气 温度 湿度 风力 水温 预报 享受运动1 晴 暖 普通 强 暖 一样 是2 晴 暖 大 强 暖 一样 是3 雨 冷 大 强 暖 变化 否4 晴 暖 大 强 冷 变化 是 天气：晴，阴， 雨温度：暖，冷湿度：普通，大风力：强，弱水温：暖，冷预报：一样，变化享受运动：是，否概念定义在实例(instance)集合之上，这个集合表示为X。（X：所有可能的日子，每个日子的值由 天气，温度，湿度，风力，水温，预报6个属性表示。待学习的概念或目标函数成为目标概念（target concept), 记做c。c(x) = 1, 当享受运动时， c(x) = 0 当不享受运动时，c(x)也可叫做yx: 每一个实例X: 样例, 所有实例的集合学习目标：f: X -&gt; Y 训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集特征向量(features/feature vector)：属性的集合，通常用一个向量来表示，附属于一个实例标记(label): c(x), 实例类别的标记正例(positive example)反例(negative example) 例子：研究美国硅谷房价影响房价的两个重要因素：面积(平方米），学区（评分1-10）样例 面积（平方米） 学区 （1-10） 房价 （1000$)1 100 8 10002 120 9 13003 60 6 8004 80 9 11005 95 5 850 分类 (classification): 目标标记为类别型数据(category)回归(regression): 目标标记为连续性数值 (continuous numeric value) 例子：研究肿瘤良性，恶性于尺寸，颜色的关系特征值：肿瘤尺寸，颜色标记：良性/恶性有监督学习(supervised learning)： 训练集有类别标记(class label)无监督学习(unsupervised learning)： 无类别标记(class label)半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集 机器学习步骤框架8.1 把数据拆分为训练集和测试集8.2 用训练集和训练集的特征向量来训练算法8.2 用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）100 天： 训练集10天：测试集 （不知道是否 ” 享受运动“， 知道6个属性，来预测每一天是否享受运动）]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（三） 元组 字典]]></title>
    <url>%2F2017-6-23-one%2F</url>
    <content type="text"><![CDATA[1. 元组（tuple）不可删除其中的元素可以整个删除，不可以更新，用（）创建，其余用法与list相同，有以下需要注意: 1.1 创建一个元素的Tuple 创建只有一个元素的tuple，需要用逗号结尾消除歧义 a_tuple = (2,) 1.2 Tuple中的list Tuple 是不可变 list。 一旦创建了一个 tuple 就不能以任何方式改变它。12345mixed_tuple = (1, 2, ['a', 'b'])print("mixed_tuple: " + str(mixed_tuple))mixed_tuple[2][0] = 'c'mixed_tuple[2][1] = 'd'print("mixed_tuple: " + str(mixed_tuple)) 1.3 Tuple 与 list 的相同之处定义 tuple 与定义 list 的方式相同, 除了整个元素集是用小括号包围的而不是方括号。Tuple 的元素与 list 一样按定义的次序进行排序。 Tuples 的索引与 list 一样从 0 开始, 所以一个非空 tuple 的第一个元素总是 t[0]。负数索引与 list 一样从 tuple 的尾部开始计数。与 list 一样分片 (slice) 也可以使用。注意当分割一个 list 时, 会得到一个新的 list ；当分割一个 tuple 时, 会得到一个新的 tuple。 1.4 Tuple 不存在的方法您不能向 tuple 增加元素。Tuple 没有 append 或 extend 方法。您不能从 tuple 删除元素。Tuple 没有 remove 或 pop 方法。然而, 您可以使用 in 来查看一个元素是否存在于 tuple 中。 1.5 用 Tuple 的好处Tuple 比 list 操作速度快。如果您定义了一个值的常量集，并且唯一要用它做的是不断地遍历它，请使用 tuple 代替 list。如果对不需要修改的数据进行 “写保护”，可以使代码更安全。使用 tuple 而不是 list 如同拥有一个隐含的 assert 语句，说明这一数据是常量。如果必须要改变这些值，则需要执行 tuple 到 list 的转换。 1.6 Tuple 与 list 的转换Tuple 可以转换成 list，反之亦然。内置的 tuple 函数接收一个 list，并返回一个有着相同元素的 tuple。而 list 函数接收一个 tuple 返回一个 list。从效果上看，tuple 冻结一个 list，而 list 解冻一个 tuple。 1.7 Tuple 的其他应用一次赋多值v = (‘a’, ‘b’, ‘e’)(x, y, z) = v解释：v 是一个三元素的 tuple, 并且 (x, y, z) 是一个三变量的 tuple。将一个 tuple 赋值给另一个 tuple, 会按顺序将 v 的每个值赋值给每个变量。 2 字典键(key)，对应值(value) 2.1 字典的增删改查1234567891011121314151617181920212223242526272829直接给出代码# -*- coding: utf-8 -*-#创建一个词典phone_book = &#123;'Tom': 123, "Jerry": 456, 'Kim': 789&#125;mixed_dict = &#123;"Tom": 'boy', 11: 23.5&#125;#访问词典里的值print("Tom's number is " + str(phone_book['Tom']))print('Tom is a ' + mixed_dict['Tom'])#修改词典phone_book['Tom'] = 999phone_book['Heath'] = 888print("phone_book: " + str(phone_book)) phone_book.update(&#123;'Ling':159, 'Lili':247&#125;)print("updated phone_book: " + str(phone_book)) #删除词典元素以及词典本身del phone_book['Tom']print("phone_book after deleting Tom: " + str(phone_book)) #清空词典phone_book.clear()print("after clear: " + str(phone_book))#删除词典del phone_book# print("after del: " + str(phone_book))#不允许同一个键出现两次rep_test = &#123;'Name': 'aa', 'age':5, 'Name': 'bb'&#125;print("rep_test: " + str(rep_test))#键必须不可变，所以可以用书，字符串或者元组充当，列表不行list_dict = &#123;['Name']: 'John', 'Age':13&#125;list_dict = &#123;('Name'): 'John', 'Age':13&#125; 2.2 字典的常用函数&amp;方法 cmp(dict1, dict2)：比较两个字典元素。 len(dict)：计算字典元素个数，即键的总数。 str(dict)：输出字典可打印的字符串表示。 type(variable)：返回输入的变量类型，如果变量是字典就返回字典类型。 radiansdict.keys()：以列表返回一个字典所有的键 radiansdict.values()：以列表返回字典中的所有值 radiansdict.items()：以列表返回可遍历的(键, 值) 元组数组2.3 字典的其余的一些方法1234567# 1、radiansdict.clear()：删除字典内所有元素# 2、radiansdict.copy()：返回一个字典的浅复制# 3、radiansdict.fromkeys()：创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值# 4、radiansdict.get(key, default=None)：返回指定键的值，如果值不在字典中返回default值# 5、radiansdict.has_key(key)：如果键在字典dict里返回true，否则返回false# 6、radiansdict.setdefault(key, default=None)：和get()类似, 但如果键不已经存在于字典中，将会添加键并将值设为default# 7、radiansdict.update(dict2)：把字典dict2的键/值对更新到dict里 eg:学生信息管理系统输入1：添加学生信息输入2：查找学生信息输入3：删除学生信息输入4：修改学生信息输入5：显示所有学生信息输入6：按成绩排序输入7：退出点此查看源码]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（二）字符串 列表]]></title>
    <url>%2F2017-6-21-three%2F</url>
    <content type="text"><![CDATA[1. 字符串：一串字符显示或者打印出来文字信息导出编码：# -- coding: utf-8 --单引号，双引号，三引号不可变（immutable)Format字符串age = 3name = “Tom”print(“{0} was {1} years old”.format(name, age))联合：+: print(name + “ was “ + str(age) + “ years old”)换行符: print(“What’s your name? \nTom”)补充:字符串的函数操作 1.1 find str.find(sub[, start[, end]]) Return the lowest index in the string where substring sub is found within the slice s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 if sub is not found. sub[, start[, end]]表示后两个参数可选[]内的参数可缺省三种形式 find(sub=a) find(sub=a,start=b) find(sub=a,start=b,end=c) 1.2 index str.index(sub[, start[, end]]) Like find(), but raise ValueError when the substring is not found. 以上两项若从右侧找rfind() rindex() 1.3 count str.count(sub[, start[, end]]) Return the number of non-overlapping occurrences of substring sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. 默认全部替换,若有count,则替换不超过count次 1.4 replace str.replace(old, new[, count]) Return a copy of the string with all occurrences of substring old replaced by new. If the optional argument count is given, only the first count occurrences are replaced. 1.5 split str.split(sep=None, maxsplit=-1) Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, ‘1,,2’.split(‘,’) returns [‘1’, ‘’, ‘2’]). The sep argument may consist of multiple characters (for example, ‘1&lt;&gt;2&lt;&gt;3’.split(‘&lt;&gt;’) returns [‘1’, ‘2’, ‘3’]). Splitting an empty string with a specified separator returns [‘’]. 1.6 partition str.partition(sep) Split the string at the first occurrence of sep, and return a 3-tuple containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return a 3-tuple containing the string itself, followed by two empty strings. 1.7 capitalize str.capitalize() Return a copy of the string with its first character capitalized and the rest lowercased. 1.8 title str.title() Return a titlecased version of the string where words start with an uppercase character and the remaining characters are lowercase. 1.9 其他 startswith endswith lower upper ljust rjust center 居中对齐 lstrip rstrip strip 删除两端空白字符 isalpha isdigit isalnum isspace join 2. 字面常量（literal constant):可以直接以字面的意义使用它们：如：6，2.24，3.45e-3, “This is a string”常量：不会被改变 3. 变量：储存信息属于identifieridentifier命名规则：第一个字符必须是字母或者下划线其余字符可以是字母，数字，或者下划线区分大小写如：合法：i, name_3_4, big_bang不合法：2people, this is tom, my-name, &gt;123b_c2 4. 注释：5. 缩进(Indentation)6. 列表的相关操作6.1 添加元素 s.append(x) appends x to the end of the sequence (same as s[len(s):len(s)] = [x]) s.extend(t) or s += t extends s with the contents of t (for the most part the same as s[len(s):len(s)] = t) s.insert(i, x) inserts x into s at the index given by i (same as s[i:i] = [x]) 6.2 修改元素直接对下标索引进行赋值直接给出代码 以后参考备用 6.3 查找元素 x in s True if an item of s is equal to x, else False x not in s False if an item of s is equal to x, else True s.count(x) total number of occurrences of x in s s.index(x[, i[, j]]) index of the first occurrence of x in s (at or after index i and before index j) 6.4 删除元素 del s[i:j] same as s[i:j] = [] s.pop([i]) retrieves the item at i and also removes it from s The optional argument i defaults to -1, so that by default the last item is removed and returned. s.remove(x) remove the first item from s where s[i] == x 6.5 排序元素 s.reverse() reverses the items of s in place sort(*, key=None, reverse=False) 用法传送门 列表专用函数sort 以上没特殊声明属于sequence序列通用函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#coding=utf-8'''Created on 2017.6.21@author: Administrator'''# -*- coding: utf-8 -*-#创建一个列表number_list = [1, 3, 5, 7, 9]string_list = ["abc", "bbc", "python"]mixed_list = ['python', 'java', 3, 12]#访问列表中的值second_num = number_list[1]third_string = string_list[2]fourth_mix = mixed_list[3]print("second_num: &#123;0&#125; third_string: &#123;1&#125; fourth_mix: &#123;2&#125;".format(second_num, third_string, fourth_mix))#更新列表print("number_list before: " + str(number_list))number_list[1] = 30print("number_list after: " + str(number_list))#删除列表元素print("mixed_list before delete: " + str(mixed_list))del mixed_list[2]print("mixed_list after delete: " + str(mixed_list))#Python脚本语言print(len([1,2,3])) #长度print([1,2,3] + [4,5,6]) #组合print(['Hello'] * 4) #重复print(3 in [1,2,3]) #某元素是否在列表中#列表的截取abcdlist=['a','b','c','d']print(abcdlist[1])print(abcdlist[-2])print(abcdlist[1:])# 列表操作包含以下函数:# 1、cmp(list1, list2)：比较两个列表的元素 # 2、len(list)：列表元素个数 # 3、max(list)：返回列表元素最大值 # 4、min(list)：返回列表元素最小值 # 5、list(seq)：将元组转换为列表 # 列表操作包含以下方法:# 1、list.append(obj)：在列表末尾添加新的对象# 2、list.count(obj)：统计某个元素在列表中出现的次数# 3、list.extend(seq)：在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）# 4、list.index(obj)：从列表中找出某个值第一个匹配项的索引位置# 5、list.insert(index, obj)：将对象插入列表# 6、list.pop(obj=list[-1])：移除列表中的一个元素（默认最后一个元素），并且返回该元素的值# 7、list.remove(obj)：移除列表中某个值的第一个匹配项# 8、list.reverse()：反向列表中元素# 9、list.sort([func])：对原列表进行排序 结果123456789101112second_num: 3 third_string: python fourth_mix: 12number_list before: [1, 3, 5, 7, 9]number_list after: [1, 30, 5, 7, 9]mixed_list before delete: ['python', 'java', 3, 12]mixed_list after delete: ['python', 'java', 12]3[1, 2, 3, 4, 5, 6]['Hello', 'Hello', 'Hello', 'Hello']Truebc['b', 'c', 'd'] 7. 切片切片是指对操作的对象截取一部分的操作。切片的语法表达式为： [start_index : end_index : step]其中： start_index表示起始索引 end_index表示结束索引 step表示步长，步长不能为0，且默认值为1切片操作是指按照步长，截取从起始索引到结束索引，但不包含结束索引（也就是结束索引减1）的所有元素。 python3支持切片操作的数据类型有list、tuple、string、unicode、range切片返回的结果类型与原对象类型一致切片不会改变原对象，而是重新生成了一个新的对象具体用法传送门]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记（一）变量数值类型]]></title>
    <url>%2F2017-6-21-two%2F</url>
    <content type="text"><![CDATA[官方文档地址1、Package分为内部Package和外部Package，内部Package直接可以import,外部package需要pip install安装包再进行import2、Python数据类型官方文档中给出这样一句话The principal built-in types are numerics, sequences, mappings, classes, instances and exceptions.总体：numerics(数值), sequences(序列), mappings(映射), classes(类), instances(实例), and exceptions(异常)Numeric Types: int (包含boolean), float, complexint: unlimited length; float: 实现用double in C, 可查看 sys.float_info; complex: real(实部) &amp; imaginary(虚部）,用z.real 和 z.imag来取两部分具体运算以及法则参见：https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex例子12345678910111213141516171819202122import sysa = 3b = 4c = 5.66d = 8.0e = complex(c, d)f = complex(float(a), float(b))print ("a is type" , type(a))print ("b is type" , type(b))print ("c is type" , type(c))print ("d is type" , type(d))print ("e is type" , type(e))print ("f is type" , type(f))print(a + b)print(d / c)print (b / a)print (b // a)print (e)print (e + f)print ("e's real part is: " , e.real)print ("e's imaginary part is: " , e.imag)print (sys.float_info) 输出结果为123456789101112131415a is type &lt;class 'int'&gt;b is type &lt;class 'int'&gt;c is type &lt;class 'float'&gt;d is type &lt;class 'float'&gt;e is type &lt;class 'complex'&gt;f is type &lt;class 'complex'&gt;71.41342756183745591.33333333333333331(5.66+8j)(8.66+12j)e's real part is: 5.66e's imaginary part is: 8.0sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1) PS:编译快捷键Ctrl+F11python值可以为None 空值python查看保留字方法12import keywordprint(keyword.kwlist) 堆内存和栈内存大致可以i理解为 栈内存存放引用 堆内存中存放数据附图]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python环境配置(Eclipse+Anaconda)]]></title>
    <url>%2F2017-6-21-one%2F</url>
    <content type="text"><![CDATA[本着开始学习深度学习和神经网络相关知识开始搭建python环境，按照顺序安装jre-8u101-windows，anaconda，eclipse文件，然后在eclipse中添加anacoda作为python的解释器。eclipse设置过程1.在eclipse的help-&gt;Eclipse Marketsplace中，输入PyDev，进行安装，2.PyDev安装完成后，点击eclipse中的windows-&gt;Preferences-&gt;PyDev-&gt;Interpreter-Python进行配置3.点击按钮new，找到Anaconda的安装目录中的python.exe，添加进去。File new -&gt;PyDev Module惯例Helloworld脚本123456#coding=gbk'''Created on 2017年6月21日@author: Administrator'''print('Hello World!'); 由于注释中有时间汉字给出gbk编码环境，否则会报错.PS：2018-2-4最近开始重新学习下python将会在以前的笔记进行添加以后所有代码更新到github上：helloword代码]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+网易云跟帖+自定义打赏设置]]></title>
    <url>%2F2017-6-15-two%2F</url>
    <content type="text"><![CDATA[折腾了半天终于可以设置评论了，由于本微博采用的是小众主题，之前折腾了半天也不能评论，这里记录一下网易云跟帖的设置方法 &lt;div id="cloud-tie-wrapper" class="cloud-tie-wrapper"&gt;&lt;/div&gt; &lt;script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"&gt;&lt;/script&gt; &lt;script&gt; var cloudTieConfig = { url: document.location.href, sourceId: "", productKey: "d8314b41ee2d4bbe9df7fa62fa36989e", target: "cloud-tie-wrapper" }; var yunManualLoad = true; Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s", true); &lt;/script&gt; 然后编译调试即可通过，将youyan.ejs文件section替换并没有重新建立文件在总设置中打开友言评论，文章中设置comments: true属性即可打赏设置，这里主题中有bug,反复调试后才得以发现 &lt;% if ((theme.reward_type === 2 || (theme.reward_type === 1 &amp;&amp; post.reward)) &amp;&amp; !index){ %&gt; &lt;div class="page-reward"&gt; &lt;p&gt;&lt;a href="javascript:void(0)" onclick="dashangToggle()" class="dashang"&gt;赏&lt;/a&gt;&lt;/p&gt; &lt;div class="hide_box"&gt;&lt;/div&gt; &lt;div class="shang_box"&gt; &lt;a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()"&gt;×&lt;/a&gt; &lt;div class="shang_tit"&gt; &lt;p&gt;&lt;%= theme.reward_wording1%&gt;&lt;/p&gt; &lt;/div&gt; &lt;div class="shang_payimg"&gt; &lt;img src="/img/alipayimg.jpg" alt="扫码支持" title="扫一扫" /&gt; &lt;/div&gt; &lt;div class="pay_explain"&gt;&lt;%= theme.reward_wording2%&gt;&lt;/div&gt; &lt;div class="shang_payselect"&gt; &lt;% if(theme.alipay) {%&gt; &lt;div class="pay_item checked" data-id="alipay"&gt; &lt;span class="radiobox"&gt;&lt;/span&gt; &lt;span class="pay_logo"&gt;&lt;img src="&lt;%= theme.alipay%&gt;" alt="支付宝" /&gt;&lt;/span&gt; &lt;/div&gt; &lt;% } %&gt; &lt;% if(theme.weixin) {%&gt; &lt;div class="pay_item" data-id="wechat"&gt; &lt;span class="radiobox"&gt;&lt;/span&gt; &lt;span class="pay_logo"&gt;&lt;img src="&lt;%= theme.weixin%&gt;" alt="微信" /&gt;&lt;/span&gt; &lt;/div&gt; &lt;% } %&gt; &lt;/div&gt; &lt;div class="shang_info"&gt; &lt;p&gt;打开&lt;span id="shang_pay_txt"&gt;支付宝&lt;/span&gt;扫一扫，即可进行扫码打赏哦&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.2.0/zepto.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(".pay_item").click(function(){ $(this).addClass('checked').siblings('.pay_item').removeClass('checked'); var dataid=$(this).attr('data-id'); $(".shang_payimg img").attr("src","/img/"+dataid+"img.jpg"); $("#shang_pay_txt").text(dataid=="alipay"?"支付宝":"微信"); }); function dashangToggle(){ $(".hide_box").fadeToggle(); $(".shang_box").fadeToggle(); } &lt;/script&gt; &lt;% } %&gt; bug出现在post.reward上，原主题中写的是post.toc 主题设置中reward_type设置为1,文章reward: true则可以实现打赏，没有此属性无打赏功能]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LATEX学习笔记]]></title>
    <url>%2F2017-6-15-one%2F</url>
    <content type="text"><![CDATA[首先给上一篇自己写的小短文各项内容基本都有1234567\documentclass[UTF-8]&#123;ctexart&#125;\usepackage&#123;graphicx&#125;\usepackage&#123;float&#125;\title &#123;杂谈勾股定理&#125;\author&#123;张三&#125;\date&#123;\today&#125;\bibliographystyle&#123;plain&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748\begin&#123;document&#125;\maketitle\begin&#123;abstract&#125;这是一篇关于勾股定理的小短文。\end&#123;abstract&#125;\tableofcontents\newtheorem&#123;thm&#125;&#123;定理&#125;\section&#123;勾股定理在古代&#125;西方称勾股定理为毕达哥拉斯定理，将勾股定理的发现归功于公元前 6 世纪的毕达哥拉斯学派。该学派得到了一个法则，可以求出可排成直角三角形三边的三元数组。毕达哥拉斯学派没有书面著作，该定理的严格表述和证明则见于欧几里德\footnote&#123;欧几里德,约公元前330--225年。&#125;《几何原本》的命题 47：“直角三角形斜边上的正方形等于丙直角边上的两个正方形之和。”证明是用\emph&#123;面积&#125;做的。我国《周神算经》栽商高(约公元前 12 世圮)答周公问:\begin&#123;quote&#125; \zihao&#123;-5&#125;\kaishu 勾三股四弦定五。\end&#123;quote&#125;\begin&#123;table&#125;[h] \centering \begin&#123;tabular&#125;&#123;rrr&#125; \hline 直角边$a$ &amp; 直角边$b$ &amp; 直角边$c$\\ \hline 3 &amp; 4 &amp; 5 \\ 5 &amp; 12 &amp; 13\\ \hline \end&#123;tabular&#125;\qquad$a^2+b^2=c^2$\end&#123;table&#125;图\ref&#123;fig:xiantu&#125;是我国古代对勾股定理的一种证明。\begin&#123;figure&#125;[ht] \centering \includegraphics[scale=0.6]&#123;11.png&#125; \caption &#123;宋赵爽在《周神算经》注中作的弦图(仿制)，该图给出了勾股定理的一个极具对称美的证明。&#125; \lable&#123;fig:xiantu&#125;\end&#123;figure&#125;\begin&#123;thm&#125;[勾股定理] 直角三角形的平方等于两腰的平方和。 可以用符号语言表述为:设直角三角形$ABC$,其中$\angle C=90^\circ$,则有 \begin&#123;equation&#125; AB^2=BC^2+AC^2 \end&#123;equation&#125;\end&#123;thm&#125;\section&#123;勾股定理的近代形式&#125;\bibliography&#123;math&#125;\end&#123;document&#125; 左对齐命令123&#123;\raggedright内容&#125; 花括号12345678\begin&#123;eqnarray&#125;x'_&#123;sj&#125;(k)=\begin&#123;cases&#125;-1&amp;\text&#123;$u'_&#123;sj&#125;(k)&lt;-1$&#125;\\u'_&#123;sj&#125;(k)&amp;\text&#123;$-1\le u'_&#123;sj&#125;(k)\le 1$&#125;\\1&amp;\text&#123;$u'_&#123;sj&#125;(k)&gt;1$&#125;\end&#123;cases&#125;\end&#123;eqnarray&#125; 图片插入.eps图片: 将word图片转为pdf ,再由pdf 转为ps的eps格式然后输入代码123456\begin&#123;center&#125;\includegraphics [scale=0.8,trim=0 0 0 0]&#123;2.eps&#125;\\\label&#123;Fig2&#125;&#123;\fontsize&#123;9.3pt&#125;&#123;11.6pt&#125;\selectfont 图~2~~水下航行器垂直面受力示意图\\Fig.~2~~Schematic diagram of force in vertical plane of underwater vehicle&#125;\end&#123;center&#125; 截断命令在文章中加入以下命令将文章截断显示在主页1&lt;!-- more --&gt;]]></content>
      <tags>
        <tag>LATEX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+github+个人域名搭建个人博客]]></title>
    <url>%2F2017-6-14-one%2F</url>
    <content type="text"><![CDATA[Begin:主要参考:1、http://opiece.me/2015/04/09/hexo-guide/参照这篇搭建基本环境，第一种方式不知道为什么挂掉，直接采用第二种方式SSH连接githubSSH需要在github中设置一下2、http://jingyan.baidu.com/article/dca1fa6fa1e403f1a5405262.html参照这个教程链接个人域名和github，基本上就是解析一下域名，填上github的IP，本地创建CNAME文件，编译更新上传到github就OK嗯，懒得手打教程所以有现成的全部搬过来，整个搭建过程很顺利没什么难点End]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写在前面的话]]></title>
    <url>%2Fflower%2F</url>
    <content type="text"><![CDATA[本主页主要用于记录个人学（wan）习（le）笔记，发波宝宝养的发发庆贺个人主页建成了！]]></content>
      <tags>
        <tag>多肉植物</tag>
      </tags>
  </entry>
</search>
